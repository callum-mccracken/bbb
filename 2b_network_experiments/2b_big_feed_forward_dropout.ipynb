{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# How does the shape / size of our network affect its ability to learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting data by tag\n",
      "308955\n",
      "303925\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 700)               15400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 700)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 500)               350500    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8)                 408       \n",
      "=================================================================\n",
      "Total params: 552,220\n",
      "Trainable params: 552,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 212747 samples, validate on 30393 samples\n",
      "Epoch 1/200\n",
      "212747/212747 [==============================] - 10s 48us/step - loss: 1.1592 - acc: 0.4477 - val_loss: 1.1027 - val_acc: 0.4795\n",
      "Epoch 2/200\n",
      "212747/212747 [==============================] - 10s 46us/step - loss: 1.1109 - acc: 0.4755 - val_loss: 1.0865 - val_acc: 0.4914\n",
      "Epoch 3/200\n",
      "212747/212747 [==============================] - 10s 47us/step - loss: 1.0982 - acc: 0.4923 - val_loss: 1.0745 - val_acc: 0.5109\n",
      "Epoch 4/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0899 - acc: 0.5002 - val_loss: 1.0702 - val_acc: 0.5091\n",
      "Epoch 5/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0856 - acc: 0.5023 - val_loss: 1.0665 - val_acc: 0.5114\n",
      "Epoch 6/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0830 - acc: 0.5039 - val_loss: 1.0700 - val_acc: 0.5118\n",
      "Epoch 7/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0797 - acc: 0.5043 - val_loss: 1.0701 - val_acc: 0.5114\n",
      "Epoch 8/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0784 - acc: 0.5058 - val_loss: 1.0653 - val_acc: 0.5163\n",
      "Epoch 9/200\n",
      "212747/212747 [==============================] - 10s 45us/step - loss: 1.0775 - acc: 0.5063 - val_loss: 1.0641 - val_acc: 0.5182\n",
      "Epoch 10/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0751 - acc: 0.5070 - val_loss: 1.0587 - val_acc: 0.5191\n",
      "Epoch 11/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0740 - acc: 0.5070 - val_loss: 1.0609 - val_acc: 0.5147\n",
      "Epoch 12/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0735 - acc: 0.5072 - val_loss: 1.0594 - val_acc: 0.5162\n",
      "Epoch 13/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0730 - acc: 0.5072 - val_loss: 1.0628 - val_acc: 0.5146\n",
      "Epoch 14/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0726 - acc: 0.5086 - val_loss: 1.0641 - val_acc: 0.5152\n",
      "Epoch 15/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0721 - acc: 0.5071 - val_loss: 1.0597 - val_acc: 0.5181\n",
      "Epoch 16/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0712 - acc: 0.5070 - val_loss: 1.0600 - val_acc: 0.5163\n",
      "Epoch 17/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0711 - acc: 0.5071 - val_loss: 1.0616 - val_acc: 0.5174\n",
      "Epoch 18/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0706 - acc: 0.5078 - val_loss: 1.0596 - val_acc: 0.5164\n",
      "Epoch 19/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0706 - acc: 0.5084 - val_loss: 1.0614 - val_acc: 0.5201\n",
      "Epoch 20/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0700 - acc: 0.5100 - val_loss: 1.0628 - val_acc: 0.5135\n",
      "Epoch 21/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0704 - acc: 0.5081 - val_loss: 1.0626 - val_acc: 0.5149\n",
      "Epoch 22/200\n",
      "212747/212747 [==============================] - 10s 45us/step - loss: 1.0698 - acc: 0.5077 - val_loss: 1.0636 - val_acc: 0.5133\n",
      "Epoch 23/200\n",
      "212747/212747 [==============================] - 10s 45us/step - loss: 1.0694 - acc: 0.5071 - val_loss: 1.0612 - val_acc: 0.5119\n",
      "Epoch 24/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0682 - acc: 0.5092 - val_loss: 1.0591 - val_acc: 0.5146\n",
      "Epoch 25/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0684 - acc: 0.5082 - val_loss: 1.0608 - val_acc: 0.5184\n",
      "Epoch 26/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0687 - acc: 0.5084 - val_loss: 1.0615 - val_acc: 0.5192\n",
      "Epoch 27/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0674 - acc: 0.5096 - val_loss: 1.0563 - val_acc: 0.5175\n",
      "Epoch 28/200\n",
      "212747/212747 [==============================] - 9s 41us/step - loss: 1.0681 - acc: 0.5094 - val_loss: 1.0568 - val_acc: 0.5193\n",
      "Epoch 29/200\n",
      "212747/212747 [==============================] - 9s 41us/step - loss: 1.0678 - acc: 0.5085 - val_loss: 1.0574 - val_acc: 0.5200\n",
      "Epoch 30/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0678 - acc: 0.5094 - val_loss: 1.0606 - val_acc: 0.5160\n",
      "Epoch 31/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0673 - acc: 0.5097 - val_loss: 1.0612 - val_acc: 0.5140\n",
      "Epoch 32/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0673 - acc: 0.5091 - val_loss: 1.0650 - val_acc: 0.5153\n",
      "Epoch 33/200\n",
      "212747/212747 [==============================] - 9s 45us/step - loss: 1.0670 - acc: 0.5099 - val_loss: 1.0564 - val_acc: 0.5183\n",
      "Epoch 34/200\n",
      "212747/212747 [==============================] - 10s 45us/step - loss: 1.0672 - acc: 0.5089 - val_loss: 1.0641 - val_acc: 0.5183\n",
      "Epoch 35/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0683 - acc: 0.5085 - val_loss: 1.0606 - val_acc: 0.5167\n",
      "Epoch 36/200\n",
      "212747/212747 [==============================] - 10s 45us/step - loss: 1.0677 - acc: 0.5082 - val_loss: 1.0648 - val_acc: 0.5153\n",
      "Epoch 37/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0671 - acc: 0.5091 - val_loss: 1.0555 - val_acc: 0.5200\n",
      "Epoch 38/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0668 - acc: 0.5098 - val_loss: 1.0641 - val_acc: 0.5156\n",
      "Epoch 39/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0664 - acc: 0.5089 - val_loss: 1.0616 - val_acc: 0.5189\n",
      "Epoch 40/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0662 - acc: 0.5106 - val_loss: 1.0553 - val_acc: 0.5172\n",
      "Epoch 41/200\n",
      "212747/212747 [==============================] - 9s 45us/step - loss: 1.0666 - acc: 0.5095 - val_loss: 1.0621 - val_acc: 0.5190\n",
      "Epoch 42/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0668 - acc: 0.5094 - val_loss: 1.0538 - val_acc: 0.5205\n",
      "Epoch 43/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0666 - acc: 0.5095 - val_loss: 1.0580 - val_acc: 0.5164\n",
      "Epoch 44/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0661 - acc: 0.5093 - val_loss: 1.0625 - val_acc: 0.5175\n",
      "Epoch 45/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0661 - acc: 0.5092 - val_loss: 1.0609 - val_acc: 0.5176\n",
      "Epoch 46/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0655 - acc: 0.5096 - val_loss: 1.0558 - val_acc: 0.5191\n",
      "Epoch 47/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0654 - acc: 0.5108 - val_loss: 1.0629 - val_acc: 0.5187\n",
      "Epoch 48/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0659 - acc: 0.5093 - val_loss: 1.0605 - val_acc: 0.5180\n",
      "Epoch 49/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0652 - acc: 0.5095 - val_loss: 1.0616 - val_acc: 0.5162\n",
      "Epoch 50/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0650 - acc: 0.5097 - val_loss: 1.0561 - val_acc: 0.5189\n",
      "Epoch 51/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0647 - acc: 0.5112 - val_loss: 1.0574 - val_acc: 0.5190\n",
      "Epoch 52/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0652 - acc: 0.5102 - val_loss: 1.0592 - val_acc: 0.5200\n",
      "Epoch 53/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0662 - acc: 0.5111 - val_loss: 1.0607 - val_acc: 0.5178\n",
      "Epoch 54/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0651 - acc: 0.5107 - val_loss: 1.0595 - val_acc: 0.5186\n",
      "Epoch 55/200\n",
      "212747/212747 [==============================] - 9s 45us/step - loss: 1.0645 - acc: 0.5118 - val_loss: 1.0588 - val_acc: 0.5164\n",
      "Epoch 56/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0652 - acc: 0.5111 - val_loss: 1.0630 - val_acc: 0.5189\n",
      "Epoch 57/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0653 - acc: 0.5116 - val_loss: 1.0582 - val_acc: 0.5206\n",
      "Epoch 58/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0650 - acc: 0.5101 - val_loss: 1.0610 - val_acc: 0.5194\n",
      "Epoch 59/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0651 - acc: 0.5109 - val_loss: 1.0598 - val_acc: 0.5189\n",
      "Epoch 60/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0646 - acc: 0.5108 - val_loss: 1.0578 - val_acc: 0.5180\n",
      "Epoch 61/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0644 - acc: 0.5106 - val_loss: 1.0613 - val_acc: 0.5158\n",
      "Epoch 62/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0637 - acc: 0.5120 - val_loss: 1.0552 - val_acc: 0.5202\n",
      "Epoch 63/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0642 - acc: 0.5111 - val_loss: 1.0564 - val_acc: 0.5196\n",
      "Epoch 64/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0644 - acc: 0.5108 - val_loss: 1.0555 - val_acc: 0.5205\n",
      "Epoch 65/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0648 - acc: 0.5120 - val_loss: 1.0557 - val_acc: 0.5203\n",
      "Epoch 66/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0642 - acc: 0.5109 - val_loss: 1.0569 - val_acc: 0.5176\n",
      "Epoch 67/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0649 - acc: 0.5121 - val_loss: 1.0584 - val_acc: 0.5172\n",
      "Epoch 68/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0647 - acc: 0.5110 - val_loss: 1.0574 - val_acc: 0.5175\n",
      "Epoch 69/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0647 - acc: 0.5115 - val_loss: 1.0613 - val_acc: 0.5178\n",
      "Epoch 70/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0662 - acc: 0.5112 - val_loss: 1.0640 - val_acc: 0.5170\n",
      "Epoch 71/200\n",
      "212747/212747 [==============================] - 10s 45us/step - loss: 1.0636 - acc: 0.5103 - val_loss: 1.0553 - val_acc: 0.5193\n",
      "Epoch 72/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0643 - acc: 0.5112 - val_loss: 1.0617 - val_acc: 0.5184\n",
      "Epoch 73/200\n",
      "212747/212747 [==============================] - 10s 45us/step - loss: 1.0645 - acc: 0.5100 - val_loss: 1.0555 - val_acc: 0.5204\n",
      "Epoch 74/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0634 - acc: 0.5131 - val_loss: 1.0597 - val_acc: 0.5181\n",
      "Epoch 75/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0643 - acc: 0.5128 - val_loss: 1.0598 - val_acc: 0.5220\n",
      "Epoch 76/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0696 - acc: 0.5119 - val_loss: 1.0589 - val_acc: 0.5204\n",
      "Epoch 77/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0646 - acc: 0.5116 - val_loss: 1.0587 - val_acc: 0.5154\n",
      "Epoch 78/200\n",
      "212747/212747 [==============================] - 10s 45us/step - loss: 1.0650 - acc: 0.5118 - val_loss: 1.0669 - val_acc: 0.5166\n",
      "Epoch 79/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0636 - acc: 0.5122 - val_loss: 1.0570 - val_acc: 0.5210\n",
      "Epoch 80/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0645 - acc: 0.5115 - val_loss: 1.0586 - val_acc: 0.5192\n",
      "Epoch 81/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0642 - acc: 0.5113 - val_loss: 1.0562 - val_acc: 0.5204\n",
      "Epoch 82/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0640 - acc: 0.5120 - val_loss: 1.0583 - val_acc: 0.5189\n",
      "Epoch 83/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0636 - acc: 0.5123 - val_loss: 1.0571 - val_acc: 0.5184\n",
      "Epoch 84/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0634 - acc: 0.5124 - val_loss: 1.0589 - val_acc: 0.5167\n",
      "Epoch 85/200\n",
      "212747/212747 [==============================] - 9s 41us/step - loss: 1.0644 - acc: 0.5119 - val_loss: 1.0564 - val_acc: 0.5209\n",
      "Epoch 86/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0646 - acc: 0.5110 - val_loss: 1.0577 - val_acc: 0.5194\n",
      "Epoch 87/200\n",
      "212747/212747 [==============================] - 9s 41us/step - loss: 1.0639 - acc: 0.5119 - val_loss: 1.0550 - val_acc: 0.5201\n",
      "Epoch 88/200\n",
      "212747/212747 [==============================] - 9s 41us/step - loss: 1.0631 - acc: 0.5117 - val_loss: 1.0544 - val_acc: 0.5192\n",
      "Epoch 89/200\n",
      "212747/212747 [==============================] - 9s 41us/step - loss: 1.0640 - acc: 0.5105 - val_loss: 1.0554 - val_acc: 0.5163\n",
      "Epoch 90/200\n",
      "212747/212747 [==============================] - 9s 41us/step - loss: 1.0635 - acc: 0.5118 - val_loss: 1.0611 - val_acc: 0.5161\n",
      "Epoch 91/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0659 - acc: 0.5115 - val_loss: 1.0570 - val_acc: 0.5192\n",
      "Epoch 92/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0637 - acc: 0.5115 - val_loss: 1.0540 - val_acc: 0.5178\n",
      "Epoch 93/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0639 - acc: 0.5120 - val_loss: 1.0578 - val_acc: 0.5164\n",
      "Epoch 94/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0626 - acc: 0.5120 - val_loss: 1.0592 - val_acc: 0.5190\n",
      "Epoch 95/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0635 - acc: 0.5117 - val_loss: 1.0631 - val_acc: 0.5188\n",
      "Epoch 96/200\n",
      "212747/212747 [==============================] - 9s 41us/step - loss: 1.0632 - acc: 0.5126 - val_loss: 1.0605 - val_acc: 0.5201\n",
      "Epoch 97/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0854 - acc: 0.5123 - val_loss: 1.0573 - val_acc: 0.5177\n",
      "Epoch 98/200\n",
      "212747/212747 [==============================] - 9s 41us/step - loss: 1.0646 - acc: 0.5110 - val_loss: 1.0604 - val_acc: 0.5188\n",
      "Epoch 99/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0640 - acc: 0.5119 - val_loss: 1.0550 - val_acc: 0.5231\n",
      "Epoch 100/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0642 - acc: 0.5113 - val_loss: 1.0562 - val_acc: 0.5213\n",
      "Epoch 101/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0649 - acc: 0.5109 - val_loss: 1.0568 - val_acc: 0.5196\n",
      "Epoch 102/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0634 - acc: 0.5119 - val_loss: 1.0545 - val_acc: 0.5205\n",
      "Epoch 103/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0639 - acc: 0.5114 - val_loss: 1.0543 - val_acc: 0.5207\n",
      "Epoch 104/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0639 - acc: 0.5122 - val_loss: 1.0551 - val_acc: 0.5206\n",
      "Epoch 105/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0632 - acc: 0.5119 - val_loss: 1.0569 - val_acc: 0.5197\n",
      "Epoch 106/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0640 - acc: 0.5122 - val_loss: 1.0582 - val_acc: 0.5187\n",
      "Epoch 107/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0655 - acc: 0.5114 - val_loss: 1.0599 - val_acc: 0.5203\n",
      "Epoch 108/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0655 - acc: 0.5101 - val_loss: 1.0607 - val_acc: 0.5188\n",
      "Epoch 109/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0646 - acc: 0.5119 - val_loss: 1.0621 - val_acc: 0.5202\n",
      "Epoch 110/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0656 - acc: 0.5116 - val_loss: 1.0597 - val_acc: 0.5188\n",
      "Epoch 111/200\n",
      "212747/212747 [==============================] - 9s 41us/step - loss: 1.0645 - acc: 0.5116 - val_loss: 1.0599 - val_acc: 0.5193\n",
      "Epoch 112/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0645 - acc: 0.5120 - val_loss: 1.0551 - val_acc: 0.5193\n",
      "Epoch 113/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0635 - acc: 0.5126 - val_loss: 1.0564 - val_acc: 0.5186\n",
      "Epoch 114/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0646 - acc: 0.5124 - val_loss: 1.0576 - val_acc: 0.5181\n",
      "Epoch 115/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0648 - acc: 0.5111 - val_loss: 1.0590 - val_acc: 0.5223\n",
      "Epoch 116/200\n",
      "212747/212747 [==============================] - 9s 45us/step - loss: 1.0635 - acc: 0.5115 - val_loss: 1.0606 - val_acc: 0.5194\n",
      "Epoch 117/200\n",
      "212747/212747 [==============================] - 10s 47us/step - loss: 1.0644 - acc: 0.5118 - val_loss: 1.0571 - val_acc: 0.5194\n",
      "Epoch 118/200\n",
      "212747/212747 [==============================] - 10s 47us/step - loss: 1.0642 - acc: 0.5118 - val_loss: 1.0586 - val_acc: 0.5209\n",
      "Epoch 119/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0637 - acc: 0.5118 - val_loss: 1.0522 - val_acc: 0.5207\n",
      "Epoch 120/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0640 - acc: 0.5123 - val_loss: 1.0581 - val_acc: 0.5199\n",
      "Epoch 121/200\n",
      "212747/212747 [==============================] - 10s 45us/step - loss: 1.0644 - acc: 0.5119 - val_loss: 1.0581 - val_acc: 0.5191\n",
      "Epoch 122/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0629 - acc: 0.5127 - val_loss: 1.0591 - val_acc: 0.5201\n",
      "Epoch 123/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0642 - acc: 0.5118 - val_loss: 1.0605 - val_acc: 0.5195\n",
      "Epoch 124/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0638 - acc: 0.5116 - val_loss: 1.0568 - val_acc: 0.5182\n",
      "Epoch 125/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0649 - acc: 0.5113 - val_loss: 1.0578 - val_acc: 0.5186\n",
      "Epoch 126/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0644 - acc: 0.5112 - val_loss: 1.0595 - val_acc: 0.5190\n",
      "Epoch 127/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0639 - acc: 0.5120 - val_loss: 1.0600 - val_acc: 0.5194\n",
      "Epoch 128/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0648 - acc: 0.5104 - val_loss: 1.0587 - val_acc: 0.5167\n",
      "Epoch 129/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0635 - acc: 0.5103 - val_loss: 1.0613 - val_acc: 0.5178\n",
      "Epoch 130/200\n",
      "212747/212747 [==============================] - 9s 41us/step - loss: 1.0644 - acc: 0.5118 - val_loss: 1.0560 - val_acc: 0.5183\n",
      "Epoch 131/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0642 - acc: 0.5128 - val_loss: 1.0591 - val_acc: 0.5191\n",
      "Epoch 132/200\n",
      "212747/212747 [==============================] - 9s 42us/step - loss: 1.0643 - acc: 0.5115 - val_loss: 1.0628 - val_acc: 0.5129\n",
      "Epoch 133/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0638 - acc: 0.5125 - val_loss: 1.0620 - val_acc: 0.5178\n",
      "Epoch 134/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0639 - acc: 0.5117 - val_loss: 1.0583 - val_acc: 0.5192\n",
      "Epoch 135/200\n",
      "212747/212747 [==============================] - 9s 44us/step - loss: 1.0635 - acc: 0.5125 - val_loss: 1.0571 - val_acc: 0.5193\n",
      "Epoch 136/200\n",
      "212747/212747 [==============================] - 9s 43us/step - loss: 1.0630 - acc: 0.5124 - val_loss: 1.0587 - val_acc: 0.5202\n",
      "Epoch 137/200\n",
      "130600/212747 [=================>............] - ETA: 3s - loss: 1.0639 - acc: 0.5131"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "sys.path.append(os.path.realpath(\"..\"))\n",
    "import ptetaphi_nn\n",
    "import tools\n",
    "with open(\"/home/cmccracken/start_tf/bbb/filepath.txt\", 'r') as f:\n",
    "    filename = f.read()\n",
    "    \n",
    "s_table = tools.open_file(filename, sort_by=\"tag\")\n",
    "# filter by realistic situation where we have 3 tags and 3 or 4 jets.\n",
    "# ignore the case where there may be >4 since those are pretty rare\n",
    "nb4 = (s_table.nbjets == 3) | (s_table.nbjets == 4) # 3 or 4 b-jets exist\n",
    "nt3 = s_table.nbtags==3  # 3 b tags\n",
    "nb4nt3 = nb4 & nt3\n",
    "events = s_table[nb4nt3]\n",
    "print(len(events))\n",
    "\n",
    "# and ensure that the 3 tags are actually correct\n",
    "# this results in very little event loss\n",
    "events = events[events.truth[:,0] == 1]\n",
    "events = events[events.truth[:,1] == 1]\n",
    "events = events[events.truth[:,2] == 1]\n",
    "print(len(events))\n",
    "\n",
    "cutoff = 10  # not many events have >10 jets\n",
    "# \"pad\" = ensure all events have same length, cut off ends if needed\n",
    "events = tools.pad(events, cutoff)\n",
    "\n",
    "import importlib\n",
    "importlib.reload(ptetaphi_nn)\n",
    "nn = ptetaphi_nn.PtEtaPhiNN(events)\n",
    "# Feed forward NN\n",
    "\n",
    "# create network\n",
    "nn.model = Sequential([\n",
    "    Dense(3*(cutoff-3), input_dim=3*(cutoff-3), kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(700, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(500, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(300, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(8, kernel_initializer='normal', activation='softmax')])\n",
    "nn.model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=\"adam\", metrics=['acc'])\n",
    "nn.model.summary()\n",
    "nn.learn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.evaluate(savename=\"big_ff_dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.model.save(\"model\", save_format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
