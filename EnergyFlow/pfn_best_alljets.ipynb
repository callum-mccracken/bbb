{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.realpath(\"..\"))\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import numpy as np\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import tools\n",
    "import ef_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding arrays\n",
      "done padding\n",
      "777150 events total\n",
      "444528 events after ensuring first 3 are correctly tagged\n",
      "307055 events after ensuring we only have 3 tags\n",
      "303931 events after ensuring there is at most 1 untagged jet\n",
      "(303931, 11)\n",
      "Loaded jets\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((100,20,3))\n",
    "x = x[:,:,[0,1,2]]\n",
    "\n",
    "# load data\n",
    "if False: #os.path.exists(\"X.npy\"):\n",
    "    X = np.load(\"X.npy\", allow_pickle=True)\n",
    "    y = np.load(\"y.npy\", allow_pickle=True)\n",
    "else:\n",
    "    X, y = ef_tools.open_file(\"/fast_scratch/atlas_bbbb/MAR20p0/user.jagrundy.20736236._000001.MiniNTuple.root\", njets=10)\n",
    "    X = X[:,:,[0,1,2]]#,4]]\n",
    "    np.save(\"X.npy\", X, allow_pickle=True)\n",
    "    np.save(\"y.npy\", y, allow_pickle=True)\n",
    "\n",
    "    \n",
    "# convert labels to categorical\n",
    "Y=y\n",
    "#Y = to_categorical(y, num_classes=2)\n",
    "print(Y.shape)\n",
    "print('Loaded jets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303931, 10, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 303931/303931 [00:21<00:00, 13933.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocess by centering jets and normalizing pts\n",
    "for x in tqdm(X):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()\n",
    "\n",
    "print('Finished preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done train/val/test split\n",
      "Model summary:\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 700)    2800        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 700)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 700)    490700      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 700)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 700)    490700      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, 700)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_3 (TimeDistributed)       (None, None, 700)    490700      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, 700)    0           tdist_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 700)          0           mask[0][0]                       \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 700)          490700      sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 700)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 700)          490700      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 700)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 500)          350500      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 500)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 300)          150300      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 300)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 100)          30100       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 100)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 8)            808         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,988,008\n",
      "Trainable params: 2,988,008\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Y = y[:,3:]  # chop off first 3 jets which are always 1\n",
    "\n",
    "# do train/val/test split \n",
    "(X_train, X_val, X_test,\n",
    " Y_train, Y_val, Y_test) = data_split(X, Y, val=0.1, test=0.2)\n",
    "\n",
    "if False:\n",
    "    plt.title(\"y\")\n",
    "    plt.hist(y.argmax(axis=1), bins=15)\n",
    "    plt.show()\n",
    "    plt.title(\"Y\")\n",
    "    plt.hist(Y.argmax(axis=1)+3, bins=15)\n",
    "    plt.show()\n",
    "    plt.title(\"Y_train\")\n",
    "    plt.hist(Y_train.argmax(axis=1)+3, bins=15)\n",
    "    plt.show()\n",
    "    plt.title(\"Y_val\")\n",
    "    plt.hist(Y_val.argmax(axis=1)+3, bins=15)\n",
    "    plt.show()\n",
    "    plt.title(\"Y_test\")\n",
    "    plt.hist(Y_test.argmax(axis=1)+3, bins=15)\n",
    "    plt.show()\n",
    "\n",
    "print('Done train/val/test split')\n",
    "print('Model summary:')\n",
    "\n",
    "# build architecture\n",
    "# should we use optimizer Adam(lr=5e-5)?\n",
    "pfn = PFN(input_dim=X.shape[2], output_dim=Y.shape[1], Phi_sizes=(700, 700, 700, 700), F_sizes=(700, 700, 500, 300, 100), Phi_l2_regs=0.0001, F_l2_regs=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 212752 samples, validate on 30393 samples\n",
      "Epoch 1/300\n",
      "212752/212752 [==============================] - 25s 116us/step - loss: 2.2571 - acc: 0.4790 - val_loss: 1.8053 - val_acc: 0.5170\n",
      "Epoch 2/300\n",
      "212752/212752 [==============================] - 22s 105us/step - loss: 1.7655 - acc: 0.5167 - val_loss: 1.7179 - val_acc: 0.5212\n",
      "Epoch 3/300\n",
      "212752/212752 [==============================] - 22s 103us/step - loss: 1.6867 - acc: 0.5244 - val_loss: 1.6488 - val_acc: 0.5242\n",
      "Epoch 4/300\n",
      "212752/212752 [==============================] - 22s 102us/step - loss: 1.6189 - acc: 0.5301 - val_loss: 1.5786 - val_acc: 0.5353\n",
      "Epoch 5/300\n",
      "212752/212752 [==============================] - 21s 100us/step - loss: 1.5511 - acc: 0.5363 - val_loss: 1.5074 - val_acc: 0.5414\n",
      "Epoch 6/300\n",
      "212752/212752 [==============================] - 21s 101us/step - loss: 1.4992 - acc: 0.5418 - val_loss: 1.4549 - val_acc: 0.5507\n",
      "Epoch 7/300\n",
      "212752/212752 [==============================] - 22s 102us/step - loss: 1.4403 - acc: 0.5482 - val_loss: 1.4084 - val_acc: 0.5529\n",
      "Epoch 8/300\n",
      "212752/212752 [==============================] - 21s 99us/step - loss: 1.3878 - acc: 0.5568 - val_loss: 1.3552 - val_acc: 0.5558\n",
      "Epoch 9/300\n",
      "212752/212752 [==============================] - 21s 100us/step - loss: 1.3379 - acc: 0.5610 - val_loss: 1.2975 - val_acc: 0.5694\n",
      "Epoch 10/300\n",
      "212752/212752 [==============================] - 21s 100us/step - loss: 1.2939 - acc: 0.5661 - val_loss: 1.2689 - val_acc: 0.5683\n",
      "Epoch 11/300\n",
      "212752/212752 [==============================] - 21s 100us/step - loss: 1.2530 - acc: 0.5692 - val_loss: 1.2298 - val_acc: 0.5695\n",
      "Epoch 12/300\n",
      "212752/212752 [==============================] - 21s 100us/step - loss: 1.2208 - acc: 0.5730 - val_loss: 1.2033 - val_acc: 0.5778\n",
      "Epoch 13/300\n",
      "212752/212752 [==============================] - 21s 100us/step - loss: 1.1852 - acc: 0.5774 - val_loss: 1.1631 - val_acc: 0.5810\n",
      "Epoch 14/300\n",
      "212752/212752 [==============================] - 21s 101us/step - loss: 1.1554 - acc: 0.5796 - val_loss: 1.1507 - val_acc: 0.5738\n",
      "Epoch 15/300\n",
      "212752/212752 [==============================] - 21s 100us/step - loss: 1.1309 - acc: 0.5804 - val_loss: 1.1247 - val_acc: 0.5806\n",
      "Epoch 16/300\n",
      "212752/212752 [==============================] - 21s 99us/step - loss: 1.1104 - acc: 0.5813 - val_loss: 1.0913 - val_acc: 0.5837\n",
      "Epoch 17/300\n",
      "212752/212752 [==============================] - 21s 99us/step - loss: 1.0911 - acc: 0.5840 - val_loss: 1.0790 - val_acc: 0.5856\n",
      "Epoch 18/300\n",
      "212752/212752 [==============================] - 21s 100us/step - loss: 1.0742 - acc: 0.5865 - val_loss: 1.0988 - val_acc: 0.5820\n",
      "Epoch 19/300\n",
      "212752/212752 [==============================] - 21s 99us/step - loss: 1.0598 - acc: 0.5872 - val_loss: 1.0591 - val_acc: 0.5843\n",
      "Epoch 20/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 1.0484 - acc: 0.5892 - val_loss: 1.0433 - val_acc: 0.5878\n",
      "Epoch 21/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 1.0405 - acc: 0.5895 - val_loss: 1.0435 - val_acc: 0.5858\n",
      "Epoch 22/300\n",
      "212752/212752 [==============================] - 21s 99us/step - loss: 1.0312 - acc: 0.5918 - val_loss: 1.0325 - val_acc: 0.5904\n",
      "Epoch 23/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 1.0235 - acc: 0.5926 - val_loss: 1.0365 - val_acc: 0.5837\n",
      "Epoch 24/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 1.0179 - acc: 0.5935 - val_loss: 1.0213 - val_acc: 0.5905\n",
      "Epoch 25/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 1.0123 - acc: 0.5950 - val_loss: 1.0203 - val_acc: 0.5886\n",
      "Epoch 26/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 1.0079 - acc: 0.5965 - val_loss: 1.0140 - val_acc: 0.5902\n",
      "Epoch 27/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 1.0038 - acc: 0.5982 - val_loss: 1.0068 - val_acc: 0.5937\n",
      "Epoch 28/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9982 - acc: 0.5988 - val_loss: 1.0041 - val_acc: 0.5953\n",
      "Epoch 29/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9963 - acc: 0.6004 - val_loss: 1.0022 - val_acc: 0.5963\n",
      "Epoch 30/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9921 - acc: 0.6006 - val_loss: 1.0013 - val_acc: 0.5978\n",
      "Epoch 31/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9905 - acc: 0.6011 - val_loss: 1.0003 - val_acc: 0.5960\n",
      "Epoch 32/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9881 - acc: 0.6014 - val_loss: 1.0024 - val_acc: 0.5942\n",
      "Epoch 33/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9844 - acc: 0.6040 - val_loss: 0.9989 - val_acc: 0.5917\n",
      "Epoch 34/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9828 - acc: 0.6047 - val_loss: 1.0011 - val_acc: 0.5913\n",
      "Epoch 35/300\n",
      "212752/212752 [==============================] - 20s 95us/step - loss: 0.9814 - acc: 0.6052 - val_loss: 1.0004 - val_acc: 0.5939\n",
      "Epoch 36/300\n",
      "212752/212752 [==============================] - 21s 99us/step - loss: 0.9791 - acc: 0.6067 - val_loss: 0.9895 - val_acc: 0.6005\n",
      "Epoch 37/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9750 - acc: 0.6077 - val_loss: 0.9933 - val_acc: 0.5960\n",
      "Epoch 38/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9728 - acc: 0.6079 - val_loss: 1.0009 - val_acc: 0.5927\n",
      "Epoch 39/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9721 - acc: 0.6086 - val_loss: 0.9829 - val_acc: 0.6036\n",
      "Epoch 40/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9693 - acc: 0.6094 - val_loss: 0.9882 - val_acc: 0.5997\n",
      "Epoch 41/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9687 - acc: 0.6094 - val_loss: 0.9931 - val_acc: 0.6002\n",
      "Epoch 42/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9676 - acc: 0.6098 - val_loss: 0.9968 - val_acc: 0.5953\n",
      "Epoch 43/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9645 - acc: 0.6109 - val_loss: 0.9914 - val_acc: 0.5980\n",
      "Epoch 44/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9641 - acc: 0.6125 - val_loss: 0.9856 - val_acc: 0.6005\n",
      "Epoch 45/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9608 - acc: 0.6129 - val_loss: 0.9848 - val_acc: 0.6026\n",
      "Epoch 46/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9599 - acc: 0.6134 - val_loss: 0.9855 - val_acc: 0.5987\n",
      "Epoch 47/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9576 - acc: 0.6151 - val_loss: 0.9864 - val_acc: 0.5991\n",
      "Epoch 48/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9565 - acc: 0.6155 - val_loss: 0.9901 - val_acc: 0.5992\n",
      "Epoch 49/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9551 - acc: 0.6164 - val_loss: 0.9853 - val_acc: 0.5978\n",
      "Epoch 50/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9543 - acc: 0.6167 - val_loss: 0.9816 - val_acc: 0.6029\n",
      "Epoch 51/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.9531 - acc: 0.6176 - val_loss: 0.9871 - val_acc: 0.6025\n",
      "Epoch 52/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9514 - acc: 0.6180 - val_loss: 0.9784 - val_acc: 0.6037\n",
      "Epoch 53/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9498 - acc: 0.6183 - val_loss: 0.9891 - val_acc: 0.5954\n",
      "Epoch 54/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9489 - acc: 0.6192 - val_loss: 0.9896 - val_acc: 0.5974\n",
      "Epoch 55/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9488 - acc: 0.6192 - val_loss: 0.9830 - val_acc: 0.6012\n",
      "Epoch 56/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9471 - acc: 0.6202 - val_loss: 0.9830 - val_acc: 0.5992\n",
      "Epoch 57/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9450 - acc: 0.6217 - val_loss: 0.9920 - val_acc: 0.5978\n",
      "Epoch 58/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9449 - acc: 0.6218 - val_loss: 0.9930 - val_acc: 0.5989\n",
      "Epoch 59/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9430 - acc: 0.6230 - val_loss: 0.9845 - val_acc: 0.5999\n",
      "Epoch 60/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9420 - acc: 0.6228 - val_loss: 0.9893 - val_acc: 0.5983\n",
      "Epoch 61/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9409 - acc: 0.6229 - val_loss: 0.9944 - val_acc: 0.5973\n",
      "Epoch 62/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9394 - acc: 0.6245 - val_loss: 0.9849 - val_acc: 0.6023\n",
      "Epoch 63/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9381 - acc: 0.6252 - val_loss: 0.9903 - val_acc: 0.5999\n",
      "Epoch 64/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9378 - acc: 0.6246 - val_loss: 0.9880 - val_acc: 0.5977\n",
      "Epoch 65/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9370 - acc: 0.6263 - val_loss: 1.0004 - val_acc: 0.5962\n",
      "Epoch 66/300\n",
      "212752/212752 [==============================] - 20s 95us/step - loss: 0.9353 - acc: 0.6269 - val_loss: 0.9834 - val_acc: 0.6007\n",
      "Epoch 67/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9334 - acc: 0.6278 - val_loss: 0.9893 - val_acc: 0.5985\n",
      "Epoch 68/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9324 - acc: 0.6288 - val_loss: 0.9937 - val_acc: 0.5942\n",
      "Epoch 69/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9314 - acc: 0.6284 - val_loss: 0.9926 - val_acc: 0.5956\n",
      "Epoch 70/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9313 - acc: 0.6292 - val_loss: 0.9892 - val_acc: 0.5981\n",
      "Epoch 71/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9304 - acc: 0.6298 - val_loss: 0.9954 - val_acc: 0.5990\n",
      "Epoch 72/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9301 - acc: 0.6307 - val_loss: 1.0068 - val_acc: 0.5932\n",
      "Epoch 73/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9282 - acc: 0.6313 - val_loss: 0.9975 - val_acc: 0.5968\n",
      "Epoch 74/300\n",
      "212752/212752 [==============================] - 21s 96us/step - loss: 0.9265 - acc: 0.6321 - val_loss: 0.9937 - val_acc: 0.5945\n",
      "Epoch 75/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9273 - acc: 0.6321 - val_loss: 0.9956 - val_acc: 0.5958\n",
      "Epoch 76/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9256 - acc: 0.6317 - val_loss: 1.0203 - val_acc: 0.5937\n",
      "Epoch 77/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9247 - acc: 0.6341 - val_loss: 1.0000 - val_acc: 0.6007\n",
      "Epoch 78/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9251 - acc: 0.6323 - val_loss: 1.0001 - val_acc: 0.5958\n",
      "Epoch 79/300\n",
      "212752/212752 [==============================] - 21s 96us/step - loss: 0.9233 - acc: 0.6347 - val_loss: 0.9946 - val_acc: 0.5971\n",
      "Epoch 80/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9220 - acc: 0.6352 - val_loss: 1.0041 - val_acc: 0.5979\n",
      "Epoch 81/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9205 - acc: 0.6362 - val_loss: 1.0154 - val_acc: 0.5945\n",
      "Epoch 82/300\n",
      "212752/212752 [==============================] - 20s 94us/step - loss: 0.9203 - acc: 0.6361 - val_loss: 1.0033 - val_acc: 0.6001\n",
      "Epoch 83/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9205 - acc: 0.6366 - val_loss: 1.0093 - val_acc: 0.5948\n",
      "Epoch 84/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9175 - acc: 0.6386 - val_loss: 1.0034 - val_acc: 0.5987\n",
      "Epoch 85/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9173 - acc: 0.6383 - val_loss: 1.0147 - val_acc: 0.5944\n",
      "Epoch 86/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9165 - acc: 0.6391 - val_loss: 1.0038 - val_acc: 0.5935\n",
      "Epoch 87/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9159 - acc: 0.6400 - val_loss: 1.0056 - val_acc: 0.5962\n",
      "Epoch 88/300\n",
      "212752/212752 [==============================] - 21s 96us/step - loss: 0.9149 - acc: 0.6395 - val_loss: 1.0096 - val_acc: 0.5942\n",
      "Epoch 89/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9139 - acc: 0.6394 - val_loss: 1.0039 - val_acc: 0.5951\n",
      "Epoch 90/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9132 - acc: 0.6405 - val_loss: 1.0165 - val_acc: 0.5945\n",
      "Epoch 91/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9128 - acc: 0.6410 - val_loss: 1.0063 - val_acc: 0.5972\n",
      "Epoch 92/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9117 - acc: 0.6412 - val_loss: 1.0080 - val_acc: 0.5921\n",
      "Epoch 93/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9113 - acc: 0.6430 - val_loss: 1.0094 - val_acc: 0.5959\n",
      "Epoch 94/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9112 - acc: 0.6422 - val_loss: 1.0135 - val_acc: 0.5942\n",
      "Epoch 95/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9100 - acc: 0.6430 - val_loss: 1.0166 - val_acc: 0.5952\n",
      "Epoch 96/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9101 - acc: 0.6435 - val_loss: 1.0137 - val_acc: 0.5919\n",
      "Epoch 97/300\n",
      "212752/212752 [==============================] - 20s 95us/step - loss: 0.9076 - acc: 0.6444 - val_loss: 1.0218 - val_acc: 0.5899\n",
      "Epoch 98/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9096 - acc: 0.6426 - val_loss: 1.0126 - val_acc: 0.5954\n",
      "Epoch 99/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9068 - acc: 0.6455 - val_loss: 1.0272 - val_acc: 0.5901\n",
      "Epoch 100/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9065 - acc: 0.6450 - val_loss: 1.0142 - val_acc: 0.5968\n",
      "Epoch 101/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9050 - acc: 0.6458 - val_loss: 1.0192 - val_acc: 0.5898\n",
      "Epoch 102/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.9042 - acc: 0.6467 - val_loss: 1.0208 - val_acc: 0.5905\n",
      "Epoch 103/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9042 - acc: 0.6473 - val_loss: 1.0158 - val_acc: 0.5944\n",
      "Epoch 104/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9032 - acc: 0.6480 - val_loss: 1.0259 - val_acc: 0.5901\n",
      "Epoch 105/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.9033 - acc: 0.6476 - val_loss: 1.0184 - val_acc: 0.5885\n",
      "Epoch 106/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9025 - acc: 0.6484 - val_loss: 1.0252 - val_acc: 0.5908\n",
      "Epoch 107/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9016 - acc: 0.6480 - val_loss: 1.0280 - val_acc: 0.5891\n",
      "Epoch 108/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9008 - acc: 0.6485 - val_loss: 1.0266 - val_acc: 0.5918\n",
      "Epoch 109/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9003 - acc: 0.6493 - val_loss: 1.0328 - val_acc: 0.5905\n",
      "Epoch 110/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.9005 - acc: 0.6489 - val_loss: 1.0337 - val_acc: 0.5932\n",
      "Epoch 111/300\n",
      "212752/212752 [==============================] - 20s 95us/step - loss: 0.8992 - acc: 0.6492 - val_loss: 1.0179 - val_acc: 0.5927\n",
      "Epoch 112/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8990 - acc: 0.6502 - val_loss: 1.0298 - val_acc: 0.5882\n",
      "Epoch 113/300\n",
      "212752/212752 [==============================] - 20s 95us/step - loss: 0.8985 - acc: 0.6502 - val_loss: 1.0432 - val_acc: 0.5848\n",
      "Epoch 114/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8968 - acc: 0.6515 - val_loss: 1.0312 - val_acc: 0.5943\n",
      "Epoch 115/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8978 - acc: 0.6507 - val_loss: 1.0320 - val_acc: 0.5944\n",
      "Epoch 116/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8970 - acc: 0.6516 - val_loss: 1.0369 - val_acc: 0.5953\n",
      "Epoch 117/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8961 - acc: 0.6513 - val_loss: 1.0395 - val_acc: 0.5939\n",
      "Epoch 118/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8951 - acc: 0.6532 - val_loss: 1.0365 - val_acc: 0.5892\n",
      "Epoch 119/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8938 - acc: 0.6531 - val_loss: 1.0362 - val_acc: 0.5874\n",
      "Epoch 120/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8943 - acc: 0.6531 - val_loss: 1.0380 - val_acc: 0.5889\n",
      "Epoch 121/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8926 - acc: 0.6549 - val_loss: 1.0341 - val_acc: 0.5874\n",
      "Epoch 122/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8942 - acc: 0.6528 - val_loss: 1.0349 - val_acc: 0.5842\n",
      "Epoch 123/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8928 - acc: 0.6538 - val_loss: 1.0391 - val_acc: 0.5891\n",
      "Epoch 124/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8930 - acc: 0.6549 - val_loss: 1.0329 - val_acc: 0.5897\n",
      "Epoch 125/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8912 - acc: 0.6555 - val_loss: 1.0460 - val_acc: 0.5870\n",
      "Epoch 126/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8909 - acc: 0.6553 - val_loss: 1.0410 - val_acc: 0.5876\n",
      "Epoch 127/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8897 - acc: 0.6562 - val_loss: 1.0386 - val_acc: 0.5930\n",
      "Epoch 128/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8895 - acc: 0.6574 - val_loss: 1.0428 - val_acc: 0.5907\n",
      "Epoch 129/300\n",
      "212752/212752 [==============================] - 20s 94us/step - loss: 0.8877 - acc: 0.6572 - val_loss: 1.0617 - val_acc: 0.5876\n",
      "Epoch 130/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8886 - acc: 0.6572 - val_loss: 1.0572 - val_acc: 0.5875\n",
      "Epoch 131/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8875 - acc: 0.6580 - val_loss: 1.0399 - val_acc: 0.5883\n",
      "Epoch 132/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8885 - acc: 0.6579 - val_loss: 1.0502 - val_acc: 0.5893\n",
      "Epoch 133/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8879 - acc: 0.6580 - val_loss: 1.0527 - val_acc: 0.5869\n",
      "Epoch 134/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8853 - acc: 0.6589 - val_loss: 1.0486 - val_acc: 0.5875\n",
      "Epoch 135/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8864 - acc: 0.6584 - val_loss: 1.0704 - val_acc: 0.5862\n",
      "Epoch 136/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8854 - acc: 0.6594 - val_loss: 1.0505 - val_acc: 0.5875\n",
      "Epoch 137/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8849 - acc: 0.6598 - val_loss: 1.0601 - val_acc: 0.5922\n",
      "Epoch 138/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8845 - acc: 0.6598 - val_loss: 1.0518 - val_acc: 0.5870\n",
      "Epoch 139/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8847 - acc: 0.6599 - val_loss: 1.0588 - val_acc: 0.5905\n",
      "Epoch 140/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8832 - acc: 0.6608 - val_loss: 1.0493 - val_acc: 0.5888\n",
      "Epoch 141/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8835 - acc: 0.6601 - val_loss: 1.0535 - val_acc: 0.5871\n",
      "Epoch 142/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8827 - acc: 0.6604 - val_loss: 1.0628 - val_acc: 0.5870\n",
      "Epoch 143/300\n",
      "212752/212752 [==============================] - 21s 96us/step - loss: 0.8834 - acc: 0.6610 - val_loss: 1.0601 - val_acc: 0.5835\n",
      "Epoch 144/300\n",
      "212752/212752 [==============================] - 20s 94us/step - loss: 0.8827 - acc: 0.6615 - val_loss: 1.0584 - val_acc: 0.5841\n",
      "Epoch 145/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8829 - acc: 0.6602 - val_loss: 1.0583 - val_acc: 0.5880\n",
      "Epoch 146/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8818 - acc: 0.6617 - val_loss: 1.0563 - val_acc: 0.5844\n",
      "Epoch 147/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8822 - acc: 0.6625 - val_loss: 1.0555 - val_acc: 0.5847\n",
      "Epoch 148/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8829 - acc: 0.6622 - val_loss: 1.0562 - val_acc: 0.5835\n",
      "Epoch 149/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8788 - acc: 0.6635 - val_loss: 1.0625 - val_acc: 0.5882\n",
      "Epoch 150/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8800 - acc: 0.6632 - val_loss: 1.0656 - val_acc: 0.5802\n",
      "Epoch 151/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8793 - acc: 0.6638 - val_loss: 1.0647 - val_acc: 0.5876\n",
      "Epoch 152/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8789 - acc: 0.6643 - val_loss: 1.0592 - val_acc: 0.5844\n",
      "Epoch 153/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8780 - acc: 0.6645 - val_loss: 1.0675 - val_acc: 0.5849\n",
      "Epoch 154/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8798 - acc: 0.6648 - val_loss: 1.0735 - val_acc: 0.5879\n",
      "Epoch 155/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8785 - acc: 0.6647 - val_loss: 1.0646 - val_acc: 0.5877\n",
      "Epoch 156/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8782 - acc: 0.6642 - val_loss: 1.0764 - val_acc: 0.5835\n",
      "Epoch 157/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8766 - acc: 0.6664 - val_loss: 1.0896 - val_acc: 0.5773\n",
      "Epoch 158/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8774 - acc: 0.6652 - val_loss: 1.0670 - val_acc: 0.5837\n",
      "Epoch 159/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8764 - acc: 0.6664 - val_loss: 1.0754 - val_acc: 0.5853\n",
      "Epoch 160/300\n",
      "212752/212752 [==============================] - 20s 94us/step - loss: 0.8755 - acc: 0.6675 - val_loss: 1.0729 - val_acc: 0.5824\n",
      "Epoch 161/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8760 - acc: 0.6660 - val_loss: 1.0715 - val_acc: 0.5878\n",
      "Epoch 162/300\n",
      "212752/212752 [==============================] - 21s 96us/step - loss: 0.8737 - acc: 0.6678 - val_loss: 1.0749 - val_acc: 0.5859\n",
      "Epoch 163/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8754 - acc: 0.6667 - val_loss: 1.0722 - val_acc: 0.5869\n",
      "Epoch 164/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8747 - acc: 0.6673 - val_loss: 1.0733 - val_acc: 0.5865\n",
      "Epoch 165/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8744 - acc: 0.6673 - val_loss: 1.0744 - val_acc: 0.5902\n",
      "Epoch 166/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8745 - acc: 0.6665 - val_loss: 1.0800 - val_acc: 0.5829\n",
      "Epoch 167/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8726 - acc: 0.6695 - val_loss: 1.0766 - val_acc: 0.5864\n",
      "Epoch 168/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8727 - acc: 0.6691 - val_loss: 1.0819 - val_acc: 0.5740\n",
      "Epoch 169/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8744 - acc: 0.6683 - val_loss: 1.0817 - val_acc: 0.5812\n",
      "Epoch 170/300\n",
      "212752/212752 [==============================] - 21s 96us/step - loss: 0.8721 - acc: 0.6691 - val_loss: 1.0796 - val_acc: 0.5768\n",
      "Epoch 171/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8725 - acc: 0.6687 - val_loss: 1.0926 - val_acc: 0.5832\n",
      "Epoch 172/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8719 - acc: 0.6701 - val_loss: 1.0911 - val_acc: 0.5832\n",
      "Epoch 173/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8725 - acc: 0.6693 - val_loss: 1.0875 - val_acc: 0.5790\n",
      "Epoch 174/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8719 - acc: 0.6695 - val_loss: 1.0888 - val_acc: 0.5737\n",
      "Epoch 175/300\n",
      "212752/212752 [==============================] - 20s 95us/step - loss: 0.8701 - acc: 0.6700 - val_loss: 1.0828 - val_acc: 0.5773\n",
      "Epoch 176/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8704 - acc: 0.6703 - val_loss: 1.0822 - val_acc: 0.5781\n",
      "Epoch 177/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8697 - acc: 0.6712 - val_loss: 1.0826 - val_acc: 0.5823\n",
      "Epoch 178/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8692 - acc: 0.6714 - val_loss: 1.0837 - val_acc: 0.5793\n",
      "Epoch 179/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8691 - acc: 0.6714 - val_loss: 1.0758 - val_acc: 0.5806\n",
      "Epoch 180/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8692 - acc: 0.6711 - val_loss: 1.0898 - val_acc: 0.5816\n",
      "Epoch 181/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8670 - acc: 0.6718 - val_loss: 1.0870 - val_acc: 0.5858\n",
      "Epoch 182/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8691 - acc: 0.6718 - val_loss: 1.0951 - val_acc: 0.5814\n",
      "Epoch 183/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8690 - acc: 0.6718 - val_loss: 1.0924 - val_acc: 0.5827\n",
      "Epoch 184/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8692 - acc: 0.6717 - val_loss: 1.0963 - val_acc: 0.5801\n",
      "Epoch 185/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8692 - acc: 0.6719 - val_loss: 1.0889 - val_acc: 0.5855\n",
      "Epoch 186/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8664 - acc: 0.6735 - val_loss: 1.1085 - val_acc: 0.5835\n",
      "Epoch 187/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8674 - acc: 0.6730 - val_loss: 1.0918 - val_acc: 0.5829\n",
      "Epoch 188/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8670 - acc: 0.6732 - val_loss: 1.0856 - val_acc: 0.5820\n",
      "Epoch 189/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8663 - acc: 0.6740 - val_loss: 1.0881 - val_acc: 0.5803\n",
      "Epoch 190/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8675 - acc: 0.6722 - val_loss: 1.0872 - val_acc: 0.5851\n",
      "Epoch 191/300\n",
      "212752/212752 [==============================] - 20s 94us/step - loss: 0.8648 - acc: 0.6743 - val_loss: 1.0901 - val_acc: 0.5799\n",
      "Epoch 192/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8658 - acc: 0.6748 - val_loss: 1.0984 - val_acc: 0.5812\n",
      "Epoch 193/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8657 - acc: 0.6738 - val_loss: 1.0915 - val_acc: 0.5810\n",
      "Epoch 194/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8656 - acc: 0.6745 - val_loss: 1.1061 - val_acc: 0.5804\n",
      "Epoch 195/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8642 - acc: 0.6750 - val_loss: 1.0951 - val_acc: 0.5780\n",
      "Epoch 196/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8665 - acc: 0.6743 - val_loss: 1.1002 - val_acc: 0.5839\n",
      "Epoch 197/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8656 - acc: 0.6746 - val_loss: 1.1046 - val_acc: 0.5736\n",
      "Epoch 198/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8647 - acc: 0.6749 - val_loss: 1.1117 - val_acc: 0.5718\n",
      "Epoch 199/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8636 - acc: 0.6764 - val_loss: 1.0988 - val_acc: 0.5786\n",
      "Epoch 200/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8642 - acc: 0.6756 - val_loss: 1.0952 - val_acc: 0.5778\n",
      "Epoch 201/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8620 - acc: 0.6768 - val_loss: 1.1046 - val_acc: 0.5758\n",
      "Epoch 202/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8625 - acc: 0.6772 - val_loss: 1.1113 - val_acc: 0.5791\n",
      "Epoch 203/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8632 - acc: 0.6759 - val_loss: 1.1049 - val_acc: 0.5724\n",
      "Epoch 204/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8636 - acc: 0.6756 - val_loss: 1.1069 - val_acc: 0.5815\n",
      "Epoch 205/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8622 - acc: 0.6768 - val_loss: 1.1007 - val_acc: 0.5787\n",
      "Epoch 206/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8609 - acc: 0.6778 - val_loss: 1.0997 - val_acc: 0.5806\n",
      "Epoch 207/300\n",
      "212752/212752 [==============================] - 20s 95us/step - loss: 0.8615 - acc: 0.6773 - val_loss: 1.1005 - val_acc: 0.5834\n",
      "Epoch 208/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8613 - acc: 0.6780 - val_loss: 1.1198 - val_acc: 0.5689\n",
      "Epoch 209/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8610 - acc: 0.6779 - val_loss: 1.1204 - val_acc: 0.5801\n",
      "Epoch 210/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8605 - acc: 0.6783 - val_loss: 1.1035 - val_acc: 0.5761\n",
      "Epoch 211/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8601 - acc: 0.6781 - val_loss: 1.1096 - val_acc: 0.5771\n",
      "Epoch 212/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8600 - acc: 0.6782 - val_loss: 1.1152 - val_acc: 0.5725\n",
      "Epoch 213/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8598 - acc: 0.6786 - val_loss: 1.1090 - val_acc: 0.5783\n",
      "Epoch 214/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8582 - acc: 0.6794 - val_loss: 1.1107 - val_acc: 0.5822\n",
      "Epoch 215/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8593 - acc: 0.6794 - val_loss: 1.1099 - val_acc: 0.5796\n",
      "Epoch 216/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8581 - acc: 0.6793 - val_loss: 1.1395 - val_acc: 0.5760\n",
      "Epoch 217/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8584 - acc: 0.6793 - val_loss: 1.1241 - val_acc: 0.5786\n",
      "Epoch 218/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8574 - acc: 0.6798 - val_loss: 1.1210 - val_acc: 0.5798\n",
      "Epoch 219/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8590 - acc: 0.6793 - val_loss: 1.1232 - val_acc: 0.5745\n",
      "Epoch 220/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8585 - acc: 0.6799 - val_loss: 1.1181 - val_acc: 0.5777\n",
      "Epoch 221/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8568 - acc: 0.6815 - val_loss: 1.1361 - val_acc: 0.5770\n",
      "Epoch 222/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8568 - acc: 0.6810 - val_loss: 1.1224 - val_acc: 0.5815\n",
      "Epoch 223/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8576 - acc: 0.6802 - val_loss: 1.1115 - val_acc: 0.5766\n",
      "Epoch 224/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8567 - acc: 0.6806 - val_loss: 1.1250 - val_acc: 0.5761\n",
      "Epoch 225/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8570 - acc: 0.6817 - val_loss: 1.1223 - val_acc: 0.5714\n",
      "Epoch 226/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8563 - acc: 0.6817 - val_loss: 1.1179 - val_acc: 0.5761\n",
      "Epoch 227/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8560 - acc: 0.6816 - val_loss: 1.1307 - val_acc: 0.5767\n",
      "Epoch 228/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8545 - acc: 0.6830 - val_loss: 1.1261 - val_acc: 0.5735\n",
      "Epoch 229/300\n",
      "212752/212752 [==============================] - 21s 96us/step - loss: 0.8554 - acc: 0.6823 - val_loss: 1.1359 - val_acc: 0.5778\n",
      "Epoch 230/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8542 - acc: 0.6832 - val_loss: 1.1205 - val_acc: 0.5775\n",
      "Epoch 231/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8534 - acc: 0.6829 - val_loss: 1.1277 - val_acc: 0.5801\n",
      "Epoch 232/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8542 - acc: 0.6830 - val_loss: 1.1344 - val_acc: 0.5781\n",
      "Epoch 233/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8549 - acc: 0.6822 - val_loss: 1.1219 - val_acc: 0.5783\n",
      "Epoch 234/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8522 - acc: 0.6849 - val_loss: 1.1335 - val_acc: 0.5762\n",
      "Epoch 235/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8524 - acc: 0.6844 - val_loss: 1.1354 - val_acc: 0.5793\n",
      "Epoch 236/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8516 - acc: 0.6839 - val_loss: 1.1424 - val_acc: 0.5619\n",
      "Epoch 237/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8530 - acc: 0.6838 - val_loss: 1.1328 - val_acc: 0.5767\n",
      "Epoch 238/300\n",
      "212752/212752 [==============================] - 20s 94us/step - loss: 0.8529 - acc: 0.6840 - val_loss: 1.1317 - val_acc: 0.5802\n",
      "Epoch 239/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8523 - acc: 0.6838 - val_loss: 1.1346 - val_acc: 0.5704\n",
      "Epoch 240/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8525 - acc: 0.6838 - val_loss: 1.1276 - val_acc: 0.5789\n",
      "Epoch 241/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8521 - acc: 0.6856 - val_loss: 1.1301 - val_acc: 0.5718\n",
      "Epoch 242/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8518 - acc: 0.6853 - val_loss: 1.1464 - val_acc: 0.5744\n",
      "Epoch 243/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8524 - acc: 0.6841 - val_loss: 1.1249 - val_acc: 0.5796\n",
      "Epoch 244/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8518 - acc: 0.6849 - val_loss: 1.1420 - val_acc: 0.5679\n",
      "Epoch 245/300\n",
      "212752/212752 [==============================] - 21s 96us/step - loss: 0.8499 - acc: 0.6861 - val_loss: 1.1333 - val_acc: 0.5728\n",
      "Epoch 246/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8504 - acc: 0.6866 - val_loss: 1.1289 - val_acc: 0.5747\n",
      "Epoch 247/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8518 - acc: 0.6847 - val_loss: 1.1378 - val_acc: 0.5763\n",
      "Epoch 248/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8502 - acc: 0.6865 - val_loss: 1.1320 - val_acc: 0.5764\n",
      "Epoch 249/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8499 - acc: 0.6857 - val_loss: 1.1440 - val_acc: 0.5680\n",
      "Epoch 250/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8509 - acc: 0.6865 - val_loss: 1.1344 - val_acc: 0.5724\n",
      "Epoch 251/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8487 - acc: 0.6866 - val_loss: 1.1405 - val_acc: 0.5687\n",
      "Epoch 252/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8498 - acc: 0.6867 - val_loss: 1.1350 - val_acc: 0.5795\n",
      "Epoch 253/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8510 - acc: 0.6845 - val_loss: 1.1431 - val_acc: 0.5691\n",
      "Epoch 254/300\n",
      "212752/212752 [==============================] - 20s 95us/step - loss: 0.8490 - acc: 0.6872 - val_loss: 1.1404 - val_acc: 0.5746\n",
      "Epoch 255/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8481 - acc: 0.6878 - val_loss: 1.1554 - val_acc: 0.5739\n",
      "Epoch 256/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8488 - acc: 0.6871 - val_loss: 1.1320 - val_acc: 0.5746\n",
      "Epoch 257/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8484 - acc: 0.6876 - val_loss: 1.1420 - val_acc: 0.5701\n",
      "Epoch 258/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8492 - acc: 0.6873 - val_loss: 1.1368 - val_acc: 0.5767\n",
      "Epoch 259/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8484 - acc: 0.6867 - val_loss: 1.1391 - val_acc: 0.5741\n",
      "Epoch 260/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8475 - acc: 0.6883 - val_loss: 1.1444 - val_acc: 0.5711\n",
      "Epoch 261/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8472 - acc: 0.6879 - val_loss: 1.1502 - val_acc: 0.5700\n",
      "Epoch 262/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8480 - acc: 0.6876 - val_loss: 1.1549 - val_acc: 0.5801\n",
      "Epoch 263/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8473 - acc: 0.6887 - val_loss: 1.1427 - val_acc: 0.5721\n",
      "Epoch 264/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8470 - acc: 0.6886 - val_loss: 1.1459 - val_acc: 0.5772\n",
      "Epoch 265/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8450 - acc: 0.6897 - val_loss: 1.1475 - val_acc: 0.5739\n",
      "Epoch 266/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8460 - acc: 0.6893 - val_loss: 1.1436 - val_acc: 0.5747\n",
      "Epoch 267/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8469 - acc: 0.6884 - val_loss: 1.1474 - val_acc: 0.5740\n",
      "Epoch 268/300\n",
      "212752/212752 [==============================] - 20s 92us/step - loss: 0.8459 - acc: 0.6891 - val_loss: 1.1428 - val_acc: 0.5770\n",
      "Epoch 269/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8455 - acc: 0.6901 - val_loss: 1.1459 - val_acc: 0.5769\n",
      "Epoch 270/300\n",
      "212752/212752 [==============================] - 20s 95us/step - loss: 0.8460 - acc: 0.6893 - val_loss: 1.1492 - val_acc: 0.5758\n",
      "Epoch 271/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8456 - acc: 0.6893 - val_loss: 1.1598 - val_acc: 0.5783\n",
      "Epoch 272/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8447 - acc: 0.6905 - val_loss: 1.1531 - val_acc: 0.5727\n",
      "Epoch 273/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8444 - acc: 0.6906 - val_loss: 1.1470 - val_acc: 0.5699\n",
      "Epoch 274/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8436 - acc: 0.6910 - val_loss: 1.1538 - val_acc: 0.5756\n",
      "Epoch 275/300\n",
      "212752/212752 [==============================] - 21s 96us/step - loss: 0.8459 - acc: 0.6902 - val_loss: 1.1601 - val_acc: 0.5619\n",
      "Epoch 276/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8440 - acc: 0.6899 - val_loss: 1.1538 - val_acc: 0.5735\n",
      "Epoch 277/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8431 - acc: 0.6913 - val_loss: 1.1516 - val_acc: 0.5777\n",
      "Epoch 278/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8431 - acc: 0.6908 - val_loss: 1.1740 - val_acc: 0.5744\n",
      "Epoch 279/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8437 - acc: 0.6906 - val_loss: 1.1591 - val_acc: 0.5685\n",
      "Epoch 280/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8437 - acc: 0.6912 - val_loss: 1.1592 - val_acc: 0.5779\n",
      "Epoch 281/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8421 - acc: 0.6915 - val_loss: 1.1656 - val_acc: 0.5736\n",
      "Epoch 282/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8422 - acc: 0.6925 - val_loss: 1.1619 - val_acc: 0.5723\n",
      "Epoch 283/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8412 - acc: 0.6933 - val_loss: 1.1600 - val_acc: 0.5757\n",
      "Epoch 284/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8407 - acc: 0.6925 - val_loss: 1.1606 - val_acc: 0.5688\n",
      "Epoch 285/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8415 - acc: 0.6927 - val_loss: 1.1633 - val_acc: 0.5657\n",
      "Epoch 286/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8418 - acc: 0.6924 - val_loss: 1.1699 - val_acc: 0.5667\n",
      "Epoch 287/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8406 - acc: 0.6931 - val_loss: 1.1744 - val_acc: 0.5679\n",
      "Epoch 288/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8418 - acc: 0.6928 - val_loss: 1.1770 - val_acc: 0.5732\n",
      "Epoch 289/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8421 - acc: 0.6929 - val_loss: 1.1848 - val_acc: 0.5766\n",
      "Epoch 290/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8413 - acc: 0.6931 - val_loss: 1.1686 - val_acc: 0.5716\n",
      "Epoch 291/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8393 - acc: 0.6942 - val_loss: 1.1592 - val_acc: 0.5757\n",
      "Epoch 292/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8394 - acc: 0.6938 - val_loss: 1.1640 - val_acc: 0.5734\n",
      "Epoch 293/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8393 - acc: 0.6931 - val_loss: 1.1744 - val_acc: 0.5668\n",
      "Epoch 294/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8377 - acc: 0.6941 - val_loss: 1.1792 - val_acc: 0.5775\n",
      "Epoch 295/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8392 - acc: 0.6938 - val_loss: 1.1743 - val_acc: 0.5712\n",
      "Epoch 296/300\n",
      "212752/212752 [==============================] - 21s 96us/step - loss: 0.8380 - acc: 0.6950 - val_loss: 1.1694 - val_acc: 0.5737\n",
      "Epoch 297/300\n",
      "212752/212752 [==============================] - 21s 98us/step - loss: 0.8369 - acc: 0.6949 - val_loss: 1.1784 - val_acc: 0.5656\n",
      "Epoch 298/300\n",
      "212752/212752 [==============================] - 20s 96us/step - loss: 0.8381 - acc: 0.6952 - val_loss: 1.1857 - val_acc: 0.5727\n",
      "Epoch 299/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8371 - acc: 0.6954 - val_loss: 1.1675 - val_acc: 0.5738\n",
      "Epoch 300/300\n",
      "212752/212752 [==============================] - 21s 97us/step - loss: 0.8371 - acc: 0.6956 - val_loss: 1.1771 - val_acc: 0.5693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe0b485b910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "pfn.fit(X_train, Y_train,\n",
    "          epochs=300,\n",
    "          batch_size=500,\n",
    "          validation_data=(X_val, Y_val),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdd3zU9f3A8df7LnsQMggJIey9RECmAzcOwNHirlqrbd0dtmr7q7PVLmttqavF2rrFhQtFAScoU/ZeCRAyyCbz7vP74/M97hISOGyOy3g/H4975O477j7fBL7v+3zenyHGGJRSSqnGXOEugFJKqdZJA4RSSqkmaYBQSinVJA0QSimlmqQBQimlVJM0QCillGqSBgilABH5t4g8GOSxO0TkjFCXSalw0wChlFKqSRoglGpHRCQi3GVQ7YcGCNVmOE07d4jIKhGpFJF/iUhXEXlfRMpF5CMRSQ44fpqIrBWREhFZKCKDA/YdLyLLnfNeBmIafdb5IrLSOfdLERkRZBnPE5EVIlImIjkicm+j/Sc671fi7L/G2R4rIn8WkZ0iUioinzvbJotIbhO/hzOc5/eKyGwReU5EyoBrRGSsiCxyPmOviPxdRKICzh8qIvNEZL+I7BORu0UkQ0QOiEhqwHGjRKRARCKDuXbV/miAUG3NxcCZwABgKvA+cDfQBfvv+VYAERkAvAjc7ux7D3hbRKKcm+WbwH+BFOBV531xzj0emAX8EEgFngTmiEh0EOWrBL4HdAbOA34sIhc479vTKe/fnDKNBFY65/0JGA1MdMr0C8Ab5O9kOjDb+cznAQ/wEyANmACcDtzolCER+AiYC3QD+gEfG2PygIXAjID3vQp4yRhTF2Q5VDujAUK1NX8zxuwzxuwGPgO+MsasMMZUA28AxzvHXQK8a4yZ59zg/gTEYm/A44FI4FFjTJ0xZjawJOAzbgCeNMZ8ZYzxGGOeBWqc8w7LGLPQGLPaGOM1xqzCBqlTnN2XAx8ZY150PrfIGLNSRFzA94HbjDG7nc/80hhTE+TvZJEx5k3nM6uMMcuMMYuNMfXGmB3YAOcrw/lAnjHmz8aYamNMuTHmK2ffs8CVACLiBi7DBlHVQWmAUG3NvoDnVU28TnCedwN2+nYYY7xADpDl7NttGs5UuTPgeU/gZ04TTYmIlADZznmHJSLjRGSB0zRTCvwI+00e5z22NnFaGraJq6l9wchpVIYBIvKOiOQ5zU6/C6IMAG8BQ0SkN7aWVmqM+fpblkm1AxogVHu1B3ujB0BEBHtz3A3sBbKcbT49Ap7nAL81xnQOeMQZY14M4nNfAOYA2caYJOAJwPc5OUDfJs4pBKqb2VcJxAVchxvbPBWo8ZTMjwMbgP7GmE7YJrjAMvRpquBOLewVbC3iKrT20OFpgFDt1SvAeSJyupNk/Rm2mehLYBFQD9wqIpEichEwNuDcp4EfObUBEZF4J/mcGMTnJgL7jTHVIjIW26zk8zxwhojMEJEIEUkVkZFO7WYW8IiIdBMRt4hMcHIem4AY5/MjgV8DR8qFJAJlQIWIDAJ+HLDvHSBTRG4XkWgRSRSRcQH7/wNcA0xDA0SHpwFCtUvGmI3Yb8J/w35DnwpMNcbUGmNqgYuwN8L92HzF6wHnLgWuB/4OFANbnGODcSNwv4iUA7/BBirf++4CzsUGq/3YBPVxzu6fA6uxuZD9wO8BlzGm1HnPf2JrP5VAg15NTfg5NjCVY4PdywFlKMc2H00F8oDNwKkB+7/AJseXG2MCm91UByS6YJBSKpCIzAdeMMb8M9xlUeGlAUIpdZCInADMw+ZQysNdHhVe2sSklAJARJ7FjpG4XYODAq1BKKWUaobWIJRSSjWp3UzslZaWZnr16hXuYiilVJuybNmyQmNM47E1QDsKEL169WLp0qXhLoZSSrUpItJsd2ZtYlJKKdWkkAYIEZkiIhtFZIuI3NnE/r84UyqvFJFNzpw3vn1Xi8hm53F1KMuplFLqUCFrYnLmjJmJHbWZCywRkTnGmHW+Y4wxPwk4/hacmThFJAW4BxiDnWdmmXNucajKq5RSqqFQ5iDGAluMMdsAROQl7Lz165o5/jJsUAA4G5hnjNnvnDsPmIKdOjlodXV15ObmUl1d/S2K37bExMTQvXt3IiN1bRelVMsIZYDIouE0xLnAuKYOdBZS6Q3MP8y5WU2cdwN27n569OjReDe5ubkkJibSq1cvGk7c2b4YYygqKiI3N5fevXuHuzhKqXaitSSpLwVmG2M8R3OSMeYpY8wYY8yYLl0O7aVVXV1Nampquw4OACJCampqh6gpKaWOnVAGiN3Y+fd9ujvbmnIpDZuPjubcw2rvwcGno1ynUurYCWWAWAL0F5HezhrAl2IXUmnAma8+GTtHv88HwFkikix2EfqznG1KKaUAj9ewfm8Zzy3eyQtf7QrJZ4QsB2GMqReRm7E3djcwyxizVkTuB5YaY3zB4lLswugm4Nz9IvIA/nWC7/clrNuakpISXnjhBW688cajOu/cc8/lhRdeoHPnziEqmVKqtaut9+I1hphIN8YYauq9/PrNNSzfWUxBeQ3lNfUAjOrRmcvHHZqH/V+1m8n6xowZYxqPpF6/fj2DBw8OU4msHTt2cP7557NmzZoG2+vr64mIaNn43BquVykVHGMMi7YVMbBrIqkJdpHAwooaoiJcRLldLN9VzM0vrOBAbT0XHp/F19v3k7O/ilqPl9MHpZPZOYbRPZMZ3SOF7JTYb93MLCLLjDFjmtrXbqbaaK3uvPNOtm7dysiRI4mMjCQmJobk5GQ2bNjApk2buOCCC8jJyaG6uprbbruNG264AfBPHVJRUcE555zDiSeeyJdffklWVhZvvfUWsbGxYb4ypdS3daC2np+/+g3vrc4jJT6KrM6xpCVE8dnmQgzgNQZjoHtyLGcO7srLS3JIjoviO2O6M7ZXChccf0inzpDoMAHivrfXsm5PWYu+55Bunbhn6tDDHvPwww+zZs0aVq5cycKFCznvvPNYs2bNwe6os2bNIiUlhaqqKk444QQuvvhiUlNTG7zH5s2befHFF3n66aeZMWMGr732GldeeWWLXotSquXUe7zsKDpA77R4jDG8tCSHwZmdGJSRyLur9/LMFzvYmFfGzaf2Y82eUuo8XjbklXPeiEx6psThcgk9U+OYPCCd5PgofnBSb5JiI0nvFHNMr6PDBIjWYuzYsQ3GKjz22GO88cYbAOTk5LB58+ZDAkTv3r0ZOXIkAKNHj2bHjh3HrLxKqUNV13mIiXQDUF5dx3ur93L+iG7kFlfxy9dWsb2wktKqOib1SyU1Ppo53+xpcH6PlDieumoMZwzpGtTn9e+a2OLXEIwOEyCO9E3/WImPjz/4fOHChXz00UcsWrSIuLg4Jk+e3ORYhujo6IPP3W43VVVVx6SsSik/Ywy7S6p4ZUkOf1+whRP7d+GmyX259+11rN9bxswFWzlQW4+IcO7wTDI6xfDkp1s5UOvhuhN7M6J7Epv3VTCxXyoT+rSN8VkdJkCES2JiIuXlTa/eWFpaSnJyMnFxcWzYsIHFixcf49IppXwKK2ooqqilT5d49lfW0rVTDJ9uKuAfC7dwYr80dhQdYPayXABO6p/Gil3FXPLUYhKiI7jrnEHMW7cPtyuG+6YPZVBGJwB+NLkPRRW1dOvcNnOGGiBCLDU1lUmTJjFs2DBiY2Pp2tVfpZwyZQpPPPEEgwcPZuDAgYwfPz6MJVWq/dpTUoUB0hKiqK33EuFysXp3KYUVNXTtFE1+WQ23v7ySmnovyXGRFB+oY2DXRAoqaqir97J4m+1lf83EXkzom8qZg7uyrbCS57/ayfcn9SY7JY4fntL3kM+NjnC32eAA2s21Xelo16tUU3L2H6CwooZhWUlEuIS/zNvEPxZupXNcFP3TE1izp5SU+Ch2Fh0AIMrtoktiNPHRbqYMy2TdnlJG9Uxmzso97Np/gDdvmkSnmEgKymsY3j0pzFfX8rSbq1KqXXnyk60AXDauB51i/DMYl1bVcdHjX1JQXsOI7klM6JvKk59s47RB6czfkE9hRQ3pidFU1niYefkokuMiufqZr9ldUsWfv3scF4/ufvC9fnRyX6rqPMRH29tkRtKx7UHUGmiAUEq1etV1HtwuIdLtYkt+BQ+9vwGAv8/fwrCsJFwucImQX1ZDUUUNPz1zAI99vJlVuaXMGNOdhy8awcNzN7CtoIInrhxNvdcc7IV0xbiezF2Tx3kjMht8psslB4NDR9Wxr14p1WoVlNdwzTNf0z05ls82F+LxGtwuISk2kii3i39ePYY3V+wmt7iKOo/BawyxUW7umzaUqyb0YkzPZCprPZzpdCW9+1x/82uE2/85/3f+EH45ZdDBgKH8NEAopcKmus7DYx9v5vgeyZw+KJ3Cyhrufn0N+8qqKa2qI6+smk37yjmue2eOy+5MRXU9ry3PZfrILE4e0IWTBxw6zb/PxH5pQZXB7RJiozQ4NEUDhFIqbP74wUb+9fl2ACb0SWV3SRUF5TWM65NChFv41XmDGd87lcSYCFwuO27gp2cNIClWV048FjRAKKVCat2eMp78dCteA7GRLiprPHy2uYDeXRL4JqeEK8b1YGBGIg+/v4GeqfE8MuM4xvRKafb9uh7j6SY6Mg0QrUxCQgIVFRXhLoZSQav3eNleWElqQjTz1uVRWlXHqtxSsjrHkltSxYdr84iLiiA1PooDtR4MhpP6d2Fzfjk3ndqXW07rT0ykmyvH9TxYS1CtgwYIpVTQqmo9zF6eS15pFbefMQCvMdz0/HI+Wp9PdISLmnovABmdYphbkUfnuCguG9uD288YQEp81GHfW4ND66MBIsTuvPNOsrOzuemmmwC49957iYiIYMGCBRQXF1NXV8eDDz7I9OnTw1xSpZr3j4VbeG1ZLvHREazKLQVge2El1XVe5m/I59ITsqmq83D1xF5kdY6la6cYjDFtYr4h1byOEyDevxPyVrfse2YMh3MePuwhl1xyCbfffvvBAPHKK6/wwQcfcOutt9KpUycKCwsZP34806ZN0/9MKmyMMSzetp+oCGFA10RmL8ulvLqekwd0YdHWIv74wUai3C48XsPMy0exc38lf/xgI8bAgxcM48rxPQ95T/333PZ1nAARJscffzz5+fns2bOHgoICkpOTycjI4Cc/+QmffvopLpeL3bt3s2/fPjIyMsJdXNUBFJTXUO/1khofzROfbCU20s3m/HJeWWonokuKjaS0qg6AR+ZtAuySlv+8+gQqa+rJTokDYMaYbCqq6+mVFt/0B6k2r+MEiCN80w+l7373u8yePZu8vDwuueQSnn/+eQoKCli2bBmRkZH06tWryWm+lWppZdV1XDDzC/aUVpEQFXFwTWOAq8b3xGDYkl/BnecMJi7KzYa8ckb3TCbLmXAuMI+QlhBNWkL0IZ+h2o+OEyDC6JJLLuH666+nsLCQTz75hFdeeYX09HQiIyNZsGABO3fuDHcRVTuRX1bNhrxy8sqq6ZYUy8KN+WzKr6De4yU6wsWekmryyqq5dmJv6r1eJvRJ5bfvraeypp47pgxsMK8RwIAwLVSjWgcNEMfA0KFDKS8vJysri8zMTK644gqmTp3K8OHDGTNmDIMGDQp3EVUbtK+smuS4KKIiXFTW1PPOqj3cM2ct1XXeg8e4BIZnJRHpdlFWXUd8tJuHLhrOjDHZB485vkcyB2rrDwkOSmmAOEZWr/YnyNPS0li0aFGTx+kYCNVYncfLI/M2kZ0cx0WjstheWMnu4ip+/Pwy+qQlMCwriffX7OVArYfRPZO54+yBdO0Uw8a8cnqnxTMw4/C1gI44S6kKjgYIpVqZeo+XCLfr4OuFGwt4fKGd3vrRjzaRX14DQFbnWCpq6lmwMZ8pwzL4zqjujO2dcvDc3po8Vv8jDRBKtQJrdpfy+7kbOGtoBg+/t55rJvWiqtZLTb2HjXnlpCVE8bsLh3Pf2+v4/qTepCZEce7wTA0CKqTafYDoKIN12svKgB3JzqJKfvvueqaN7Mav31xDyYE6PttciNslzFywlegIF26XHFz0/qyhGZw1VLtCq2OnXQeImJgYioqKSE1NbddBwhhDUVERMTHaltyaeb2GbYUVbN5XQWpCNLM+386H6/bx4bp9ZHSK4ZEZQ3hk3iZ+d+FwyqrrGN8nFbcIL3y9q0FSWaljpV2vSV1XV0dubm6HGGMQExND9+7diYzUniitSUVNPSUHannyk228vWoPJQfqGuy/aFQWdR7D7Wf0p2+XhDCVUnVkHXZN6sjISHr37h3uYqgOprbey679B/jPoh38Z9FORCDS7eKcYRlM6pfGkMxO7Cw6wPJdxfzkzAEkdPBlLVXrpf8ylWoBucUHyCutpnNcJD9+bjmb82135WnHdaN7cixXju9JN2c0MsCwrKRD1kBWqrXRAKHUt/D68lzW7C7jjCHpvLZsN2+t3E2919C1UzT1HsN904aSHB/F1BGZ7Tr/pdo3DRBKBWF1binPfLGddXvLKK+uZ3dJFQCzvthOTKSLqyb0ZNnOYlbllvLf68ZyUv/m10pWqq3QAKFUIwdq6ymqqEUEHnp/AzuLKtm8r4KYSDdjeiaTFBvJkG6dmDwwnS35FUzok0pSXCSlB+rYWljBqB7J4b4EpVqEBgil8DcZpSZE8e8vd1DgjFaOdNv1EY7L7szjV4witdHspf3S/T2PkuIiNTiodkUDhOrwZn2+nfvfWUeU20Wtx0vvtHhuO70/AKN7JjM4s1OYS6hUeGiAUB3GjsJK0jtFExcVwdfb9/PIvI0UVdSyOb+Cs4Z05R9XjKKyxkN8tLvBXEhKdVQaIFSHsK2ggil//YyT+qVx5fieXP+fpaQnRtM3PYFzh2dyy2n9iHC7SIrTwKCUjwYI1W7Ve7z87NVv2FtSzd6yKmrrvXy8IZ9PNxcwOLMTz18/TtdAUOowQvp1SUSmiMhGEdkiInc2c8wMEVknImtF5IWA7R4RWek85oSynKr9yCutps7jZUNeGTMXbOWtlXuo83pJS4jm0UtGktU5luOzk3nuOg0OSh1JyGoQIuIGZgJnArnAEhGZY4xZF3BMf+AuYJIxplhE0gPeosoYMzJU5VPti8dr+Gj9Pm58fjkZnWIOjlOY1C+V564bd3Cw2pRhGURHuHTwmlJBCGUT01hgizFmG4CIvARMB9YFHHM9MNMYUwxgjMkPYXlUO+NbcnPxtiJufWkFJQfqGNg1kYqaeq6Z2IshmZ04eUCXBsEgJtIdxhIr1baEMkBkATkBr3OBcY2OGQAgIl8AbuBeY8xcZ1+MiCwF6oGHjTFvNv4AEbkBuAGgR48eLVt61erU1Hv4ZGMBWcmxvLwkh/8s2nmwa+qgjER+euYAph3Xjc5xUeEuqlLtQriT1BFAf2Ay0B34VESGG2NKgJ7GmN0i0geYLyKrjTFbA082xjwFPAV2uu9jW3R1LFTXefj7/C0899VOYiLc5JX5p26/fFwPYiPdZCfHcvHo7iRqTkGpFhXKALEbCFzlpLuzLVAu8JUxpg7YLiKbsAFjiTFmN4AxZpuILASOB7aiOoxVuSX89JVv2JJfwWmD0qmp9/CLKQOpqfcytneKrp+gVIiFMkAsAfqLSG9sYLgUuLzRMW8ClwHPiEgatslpm4gkAweMMTXO9knAH0JYVtVK5Ow/wMNzN5AaH8XzX+2iS0I0/772BCYPTD/yyUqpFhWyAGGMqReRm4EPsPmFWcaYtSJyP7DUGDPH2XeWiKwDPMAdxpgiEZkIPCkiXmxX3IcDez+p9ufTTQW8uXI3gvDuqr2AXW3tnqlDSYrVpiOlwqFdLzmqWq/CihoWbMjnhF4p/OWjTcz5Zg++f4rnDs/g/unDSGs0MZ5SquV12CVHVev1i9mrmL/B9mqOi3Jz/Ul9KD1Qx8tLc5gxJluDg1KtgAYIdczUe7y8tXIP76zaw4KNBUw9rhteY/jF2QPpmRrPgdp6Th7QhVMG6GI7SrUGGiBUSL27ai9Lduzn9MHp/Ovz7SzcWEB2SiyXje3B/dOHEhkwa2pcVISu06xUK6IBQrW4bQUVLN9VwopdxTz/1S5cAv/+cgci8MAFw7hyXA+d6kKpNkADhGpRHq/hxueXsyGvHIAfntKHH53cl7V77GptuviOUm2HBgjVIoora1m3t4xPNhWwIa+cX507mIn9UhnaLQmAE/unhbmESqmjpQFC/c825pVz9ayvD06Dcf6ITH5wUm9tRlKqjdMAob6VPSVV1Hm8vP3NHv48bxMpcVE8edVoeqbGMShDm5GUag80QKij5vUarvznV+w/UMuBWg+nD0rnoYtG0CVRxy4o1Z5ogFBBMcbw3uo8VuwqptbjZVthJQAugV+fN0SDg1LtkAYIdUR5pdXc/cZq5m/IJzrCRU29l+yUWG6a3I+Kmnp6pcWHu4hKqRDQAKGaZYzhteW7ue/ttdR5vPzm/CFcM7EX6/PKSIyOpEdqXLiLqJQKIQ0Q6hALN+ZTXl3PGyt2M39DPmN7pfCH74w4WFPwdV1VSrVvGiDUQR6v4bfvrmfWF9sBiIl0cc/UIVw9oRcul3ZZVaqj0QChANhaUMG9c9by2eZCrpnYi3OHZ5KVHEtW59hwF00pFSYaIDq47YWVeLxeLpz5JQZ48IJhXDm+Z7iLpZRqBTRAdFDGGH733nr++bltToqNdPP+bSfRM1V7JCmlLA0QHczukioi3cJH6/J5+rPtXDyqO/sra5g+MkuDg1KqAQ0QHUSdx8vcNXnc+tKKg0t7ju+Twh+/M0IT0EqpJmmA6AC2FVQw48lFFFbUcnyPzpw/ohuRbmHacd00OCilmqUBop0rr67jlhdXUO81PHzRcM4bkUliTGS4i6WUagM0QLRTu0uqePj9DXyTU8Lukiqeumo0pw/uGu5iKaXaEA0Q7Ygxhvkb8pm9LJcvthTiNTAksxP3Tx/K5IHp4S6eUqqN0QDRDhhjWLKjmGcX7eDdVXtJT4zm9MFduenUfvRLTwh38ZRSbZQGiDauoLyGv368iecW7yLCJdxx9kB+eHIfItyucBdNKdXGaYBow77YUsg1z3xNncdww8l9uPm0fnTSBLRSqoVogGij9pRUcdfrq+meHMfMy0cxpJsu86mUalkaINqYfWXV3PX6ahZszMctwn+vG6fBQSkVEhog2hCP13DbSyv4JqeUm0/tx4wx2WSn6KI9SqnQ0ADRRtR7vPzfW2tZvG0/f/jOCGaMyQ53kZRS7ZwGiFbMGMPsZbm8+PUu9pRUk1dWzU2n9uW7o7uHu2hKqQ4gqAAhIq8D/wLeN8Z4Q1skVefx8vnmQv795Q4+2VTAoIxEjstO4oHRwzhjcDoiOn+SUir0gq1B/AO4FnhMRF4FnjHGbAxdsTqu4spabvjvUpbsKCYuys1904Zy1fieOqmeUuqYCypAGGM+Aj4SkSTgMud5DvA08Jwxpi6EZewwPF7DzS8u55vcUv7wnRGcPyKTuChtBVRKhUfQw21FJBW4BvgBsAL4KzAKmBeSknUwK3YVc8U/F/PFliIemD6UGWOyNTgopcIqqAAhIm8AnwFxwFRjzDRjzMvGmFsAneznf/TWyt1c9PiXbC2o5J6pQ/w9lCqLYNm/ObjCTzCqS+Gje6GiIBRFVUp1IMHWIB4zxgwxxjxkjNkbuMMYM6a5k0RkiohsFJEtInJnM8fMEJF1IrJWRF4I2H61iGx2HlcHWc42x+s1/PXjzQzO6MT8n53CtZN6+5PQH98Lb98GOz5reFJlIcz7DdQe8G+rKICnT4fHRsHnf4H1c4784blLoaqkxa5FKdW+BBsghohIZ98LEUkWkRsPd4KIuIGZwDnAEOAyERnS6Jj+wF3AJGPMUOB2Z3sKcA8wDhgL3CMiyUGWtc0or65j1hfb2VZQyfUn9z50IR9fh7EdXzTcPv9B+OKvsPE9/7at82H3UjhQaF+X5tqfuxZDfc2hH15TAbOmwLs/bZmLUUq1O8EGiOuNMQe/ahpjioHrj3DOWGCLMWabMaYWeAmY3vh9gZnO+2GMyXe2nw3MM8bsd/bNA6YEWdZWb2VOCbe8uIJJD8/nwXfX0y89gXOHZ9qdeWtg7t3g9YLHyf0vnQXr5timpv3bYcV/7fYtH/vfNHcJRCXAb/ZDSh8o3g7562HW2fDx/f7jDuy377NnOXjrYM3rULjZ7ptzC6x8MbiL+PSPthYTqCIfCrRzm1LtRbABwi0Bne+d2kHUEc7JAnICXuc62wINAAaIyBcislhEphzFuW3S/spafvDsUj7fXMDJA7rw6o8mMPe2k4iOcNsDvnkRFs+E/HVQ7rTmVebDK1fB3m/gqycAgZ4nwupXYMXzsPD3sPVjyBoFLjck94LiHbDxfXv+10/B/m2w9g340wD44Fc2oABERNsmqYp8WP4f+OAuqCn3F7gkx+ZCGls9G5Y+YwOZz9y74D/Tjy5nopRqtYLtJjMXeFlEnnRe/9DZ1hKf3x+YDHQHPhWR4cGeLCI3ADcA9OjRowWKE1pfbi3kztdWU1ZVx1s3jmdwChDbqOWsYIP9ufMLKM+D3idD12Gw+B+2GWnF8zD0QugzGXZ+Dm8FtPQNucD+TO4Nu5fD5g/t8wNF9sZdmgvRiTYAuSIgtT/0OwOWPA1dBtpzq4phyT/hxJ/Yz3rrRhh0Plz6vP9zPPVQtNXWQAo2QEwS1FfboFO+1+ZN9q2FH3wEOqhPqTYr2BrEL4EFwI+dx8fAL45wzm4gcMKg7s62QLnAHGNMnTFmO7AJGzCCORdjzFPGmDHGmDFdunQJ8lLCI6+0mh8/txy3S/j3NaMZ/MmP4LHjoWxvwwPzAwJE2V5IHwpn/w4SM+GzP0NtOYz7IQz/Dpz7J/jhZ3DKL+05PSbYn8m9oLoEdi2yx130lK0J9JkMt6ywAcdbD12HwqRbQVy2uSgqAbodDxvetU1RvvzE1vlQutvfM6p4hw0OYHtM/WUIPHEilOy025Y/a/Mhxdub+EWstuccrpZhDOxedjS/XqVUCAQVIIwxXmPM48aY7ziPJ40xniOctgToLyK9RSQKuBRo3LXmTWztARFJwzY5bQM+AM5ykuHJwFnOtjbJ6zX87NWV1NZ7+b9+HasAACAASURBVNf3RjNx5z9g01zbg+j9gDhbXQZlufaGvelDGwwSM+y38OyxUFsB3UZB9zG2aWjs9ZA5AibfBT/+Evqfad8npbf/PUdcCgPPgdu+gctfhfhUuOYdGH0NTLwVOnXzB5i4VOh7uq19fPk3WysYeSXUHbC5jOcutPmKTQGVx83On6UuoEeVz7ZPYNMHNrfha4r66D7bpLVrESz/b9OBYuP78PRptklNKRU2wc7F1B94CNsbKca33RjTp7lzjDH1InIz9sbuBmYZY9aKyP3AUmPMHPyBYB3gAe4wxhQ5n/kANsgA3G+M2X/UV9dKzPpiO19sKeKhi4bTZ81f7Q1y1Pds88/H99lv7IPO8yd4B54LG96xzxOd5HX2OFj3lq09NCZiawM+Kc6fpc9kSOtnnyf39O+PTYapf/W/PvEntvlpwBSIioPP/gSfPwK9T4HjLoGVz0Fpjn38Y7ytfQAMngp7V8E178LfRoOnBlL7QdEWiEqE7Z9AwSbIX2t7UtVWwpaP7LmvXW+D4c4vbLPWxf+CvSvhxctg2EX2mH1rIfM4u79xU1xz6qps9913fmIDaFO/L6VUUILNQTyD7Xb6F+BU7LxMR6x9GGPeA95rtO03Ac8N8FPn0fjcWcCsIMvXaq3bU8Yf5m7kzCFduTRtG7z3J/utfOpj9ka75jV47w6bCyhYb0866acBASLD/jzuMvuNfuhFR/7Q9CEw47/2PYPhcsPUR+3zumr/9nP/CAnp/tedsuxNvtrp0Dbjv+D1gDsCBk6xzWOjrrI1h/g0m8j21kFkPKx6GVyR4I60+Y8ypxvuN06vqU1zbUCoKYNVr9htBRsh52tbe7nuI+g++vDXUVcFjwy2AcUVYZPm3UZB9gnB/R6UUg0EGyBijTEfi4gYY3YC94rIMuA3RzqxI/t8cyG3vrSCpLhIfn/xCGT2xZCUDef+wX7rd0fCmffBcxfbHkbr5kBCV8gc6X8TX4CIS4GTfhbcB4vAkGnfrtCRMXDR07acvsR1ci+I6QxXvmabv57/DiR1d67B+Sc07e82gCWkw8RbbM4ib7VNWl/9NuxZYZPdB4rst/vtn8Ck2yCxG3zxqL3+2kr7Xr7mqsLNsP5tOx5k8weHBoiyvfCvMyFjOEz7m33vqmIY+0OYcBM8eZLNhxwpQMy9GxK62JrU4XjqbVkijtSBT6n2IdgAUSMiLmCz02y0G51i47Ce/2onv35zDf3TE3j8ytGkSIUd8DbpNoiK9x/Y93ToMggW/BZKdsHku+03+hn/seMXOvds/kNCZcSMhq8vetqWOT7Nvr6uid5JMZ2AgKVPk7Lg+gW2y2x8qr8JLLazbS7b/gmMuhpS+9pk9tJnbBNVoMKN/kT39k/h1Lvt8+oy2x239oBtGivNtT2vujuBYOiFtkmtxwSb6zgcrweWPWN/Hnc5JHZt/tgP7rLJ8+vnH/49lWongu3FdBt2HqZbgdHAlUC7nf7if1VyoJZX3/2Aq7KLeOuiePoWf2mbUIwHBp/f8GAROONeOFAMEbEw5lq7fch0uGWZ/UYfbtljG+Y4XK7guq9GRNng0NiEG+GK2TY4gA0UvuAQ4wzYT+phcxn562z+IXep7Vr70hW2x9WK52Dt6zYxnzEcdnzuHz2e5AyZ6TnRvkdFPs0q2GhrLJ4a25X4cDbNtQFix+ew9s0jX39LyF9vg5dSYXDEAOEMirvEGFNhjMk1xlxrjLnYGLP4GJSvzdleWMldr6/mHp7kV3WPEfvCRfDCd+HzR22zTbdRh5408Bz4+Ua4dXnDNv/2KjbZ3+MKoOsQ6HWSfT76GvtzWECu5fR7bC7jhUtsbmbZMzYZnjECTvq5PTd3iQ0G4vIn9ntMtD93LbJJcl+PqfwN8PwMW+vYs8JuSx8CK1+wzUgAO7+005T4lO62NTywifRXrz50CpSWtm+t7RSw9o3Qfo5SzThiE5MxxiMiJx6LwrR1VbUernrqCyoqSpkZtR1XCbbWALa5ZMZ/m//mHRXfsOmpo7nsJTuwb8gFMHgapA+yv4/jLoVO3e1NcvsntjmuYIPNx4y83J57oMgO/lv3lg0ObmdOq8zjIDoJFj/udAQ4E6b/3eZ8ynLtWI1B59keV6f80t70ty+0Y0be/amtzfxkre3ZFdhUVVNmf751k+1eHBVnX9dW2nPTBznl2g/bFjYMdsbY3Epyr6ZzGV6PbWIE27sNYN8aO55FqWMs2BzEChGZA7wKVPo2GmNeD0mp2iCv1/CnDzfym6qHmZyUg6vKA74u/t3H2i6h3zZx3BFEJ/hvpL5k9CkBY0QueBzmPwCn/sreiH0JdICeE0DcdqBe9jj/9ogoOPUumOtMJLzyOeicbYPDkOk2oKx8weYqBkyxweSVq+14k4wRkLcK/jPNdhqoLrUDCTt1g8JNcPZDNifx1k02Id59DHzyB9tM9bONtlPBV0/AJ7+3ie2FD8N1H8LmefDGDRCfDj/8FDpl+stbtBX+Nsrmn+K7wOpX7XbfXFlKHWPBBogYoAg4LWCbATRAAHjquf3lVXyyajN3x6zEXdWozfjEn8Cgc8NTtvYiKQsufKLpfbHJ0OtEW8Po1GjKrhOut+34/c6AhQ/ZGzbAKXdC0TaoKbWBKDIGzn/E3sC7j4Ex34d/nw+5X/vnrRr3Y9trKyIaxv/Y9tD68jE7q+4dW22OwlNrx3qMmGGbqQDe+an9nNWz7TTsMUl2fq1Nc/05J7Aj1gFe+V7Da9i3Fl7/of135KudBKuiwF5bdOLRnacUwS85eu2Rj+qgvB6qnzqDU/fEcfqg03Dv8Nium50ybfu28Tb8tqtCY/BUGyBqKxtud0fAtMfs8+LtNsEdk2Sbqm5YaHMWLicVN/w7DZtyLn/ZjlX57M9QtgfOesCOr/B6bFPhWQ/YwYjPXWTzIr55tFa9YgdB5i61r2sr7M+ls+wxk++0EyNuW2ADxP5tsOAhW0sB+xmjr7HlrNhnE/LF2wFjp00JljF2DEnWaLj4abttzq222XP6TP9x2z6xn601XNVIsCOpn8HfYHKQMeb7LV6itmbpLGL2rWCKKwq312WDw01fAQaemmyTm8m9wlzIDmDgufDez6Hvac0fM/QiGyCyxzlB4Qh9NGKcbrtn/7bhdnfAf5s+k20N5sNf29c9JsKWefbh48tDFay3gwaPu8wmvDe+Zwcmzv6+P1ne93TbxBTt9CJf8Zx9AMSlHb68jRVthf1b7dgQ31Qny5+1P4+7zNa6wP5OyvfaIBuYIyvcYhP/A4Ocad9T58//qHYh2G6u7wDvOo+PsR3eK0JVqDbD68H76Z/IN8nESi1Ruz6D46+wN5aYJJt7yBrtTzqq0EnKgjt3wdgbmj+mczac9aAdi9JSXG7/LLpDpttv+Gc/5P9SEOd08z3+SjuY8JZldoxG71PsjXvR32xwiHd6r2WN9gcHsDPu+lQ5s8146uFfZ8Ef+9s8SiCvx99ba9sC/3nLZvlH5wO8dDkse9aOU8lbZWsqpTkN32vhQ/DaD4L7PSyaCQ+k2WtS7Uawk/W9FvB4HpgBNLvUaIex4zNcFXk8WHc59VGdbBJzfMD021MfhSteDV/5OpqYJH9zUXMm3uL/5txSzrzPDgr87rM2CE24EW5cbGfa9XVrzhxppyHxJaWznO3fvGR/nvWA/Zk9tuF7Zwy3tQpXhH88x/5tkPOVzWN887L/WGNg5jg76BJsTsM3h9W7P/PnNi553ibh374V/nW2f+XC3CUNp1rJW2UnjKwua/q6ayogx8nPfOAMYiw9ZNJlmwP682CbS2nO7mX+BbJUqxFskrqx/kAH6LDfjNnXQfY48jZ+RQKx7M08HfekwRARY3uv+ETGhq+M6tiJSfLf8H0iY+1Mu76ZdVP7Ndyf0sf+eynaYsfHjLgEUvraBHmgqDi46nU7bmP/VvjLMH9PrcyRdtCep942e+Wvh6LNdlR6TJJtwhp/k52upHQXYGyX3oHn2Md7d8DSfzkfJLapC7Ej0s+4x997qmyPv7kt0ILf2l5bFwR0HqhyViwMbKpa8zqU74FF/4ALZh76PoWb7ey95//Fdg5ojtdjc0ZNdRWvq3bm+QpRbd0Y/7xjHUhQNQgRKReRMt8DeBu7RkTHU7gF1syG9+/Au3U+SyLH8PtLxyGjroIR3w136VRrkzYAkEM7KrjckD7YPk8f7EzpfkLz42QS0m0wKc2x//7EZZvTakrtLLhgx1yAXZf8w1/bua/OuMdODfKL7bYZK+t4+9kuN0x5CNzRthnLN43K6GvsiPR/n8fBtGNZo1qB12N7R/m2z7nFv++bl+G+zjb/4ePLx6x+1a5OWLwDXr4Sinc2LPfhBh7WVcNfj7NdnX0KNvrXKPlthm02C7ThXVj/DoeVv+HQZrqmbJoLf+jd4ZrQgu3FpH3kfNbMPvi0m+wnbdJkorrotFSqGcdfZQfsdep26L6uQ23+IX3Ikd+n8Qj7lL4w4Gz7fPsntuaxbaHtPeX12DzHd2bZLrkJzmJa33ur4dQtEdFwxxabRM9fb5uHRnzXrgfy9Kn+48r22G1r37BdbZf9Gz57BLo5k0p662DCzbDo73asCcC6N+1gxop8e42+cScr/mNnFagugezxMPFm2PGZPSfnq+avf+3rNjh++Tdb4+p1Eswca2tRFz0FGHsTf/9OW4s/5Rf+gHFvafPvu+hvtoYzeNrhp4/Zs9IOkCzYBD3GNX2Mp95e95ALDl/TaFzDasWC7cV0ITDfGFPqvO4MTDbGHKMJaVqRRt82ojKHNnOgUtgbcuO8gk+6828nmAAR3yhAdB1iv/Wn9oPcZfaGvWWerVWc9mvbW6pxPqZrE5/jazrqOdG/LWsURMb5Z9Ut32uD0OpX7TgOV4TNTexa7Cxbe7qdSHHR3wEBjN23Z4WtLQBMvM0uADX/t/7VCLd8ZBPnuxbZmkxpjg1SSQFjWbweeOYc2LfOJv7L98E7t/vn7CrcZGdBBlurWvcmxKYcvrNCoLK99jqrSw6/5oivtrRvjX0+9EJ7kzfGbouMsxNKvnO7bV4cdJ7/3JpyW2M6834b2B/KgvMegROuC66MPl8/bb8oDJl+dOf9D4LtxXSPLzgAGGNKsOtDdCyeekzBRjaZ7v5tOsZBfVt9JtupQXqMP/KxvlpAdJL96QsuWaPt2uQf3wcDzrEjzaMTj5ysP5LbV9tEe3wXe0Pc78yqW7zDNnWBvdH3PxPO+b2dFiW6EwebpTZ/aLt5L33GBpSMYbbJy1tne3ANnmZ7WfmmMBl7vf05/0E7S++K5+DPg2xTWc5XNiCd9Vv40Wd2BUXfmiQZI2Dju/5yl++15fONQQH/ZIebPrTzaH32SMPj4dClfze8C4+f6B9XU7bH/lzwO5h9rV3oCmzAfOJEOwL+w/+z23xdln22LbSPr57yz0783h3N/uoxxk5KuezZhts/f9T2FjuGgv1X1NRxHStbA1CWixgP8z1O1ToiJjzTcav2oesQ+NmGhqv9NcdXg+g1yc5b5buhdhvlH2B36t1NJ5O/jfg0mxtJzLTf6gs22m/lg853EsVOMjgpYOn4wA4aPr75syKi7VQq4rLroGc506l0HQa3r4EzH7AJ6m9esMHurZvszds3w+7PNtmZkNP6w4k/ha7D7fbaCmcWX/H3xvLU2CYpnwP77TXMvtYm7r941L/vYIDY49/m9dqBi/tW+xev8tUgDhTan2vftO+77Fk7en/ohTaIids2R5Xsgt9l2RUXfTmWDW/bHmhgm/VqK+GxUXaEfaD89bZmtfpVeOY8G3i8XqjIs/sOt557Cws2QCwVkUdEpK/zeAToeKvKO9XlL7zDMa4Im4DUMQ7qWEhw1qlI6297IPluxr4bbVKPhlOyt5ROWXZSw9pyW1ue/ne4+h1bDoDOPfzHxjplGjwNfrMf0pzadcYIf1l/sd1Oe+Jbu2P4d23XYJfL9mJKG2hzHOAfX5I+pOE6HRFR8OPP7fodVcX2kTG8YblXPu9/XrEP5v7Sjooffa0NqDXlNvHtSzoveBD+NBCeOhUeHe7PIXz9lL0hBwYQgCVP26T11o9h5BVw0T/h2rm2N9relfbmXlth5/rattD+bqpL7TrsPl/+zfZM841X8VnvNJntWmxrh4sfh70rbPlrymyQqHWa/8r2NN21uIUEGyBuAWqBl4GXgGrgplAVqtVyAkREen8ka3RwTQNKtYTO2dBl8KEjxTOG2/bvweeHJvGZ1N1/E+0yyJn3apJ97iuXj29QYGKm/eLUc4K/jD6xTu6gxwS7CqCvJuTTc6JdmTAq0fbAAtsU15SYJCjPszfOzBGH7s9yugwvf9Z29T3ll9DnFLuteIfteuuzZ4X9hr5nuZ3Mcc8KcEfZ9Ui2zndm8HV+v71PsTWFwVPtl8RR37MBpecEm7ivLPA3cXnrbZPXxJvtOVsDFpvyNXX51qL3Wf+2XZ7Xl6sxHvgwYPHOxyfAfy+0uadHBsOzjdaYaUHBDpSrNMbcaYwZY4w5wRhztzGm8shnti81BduoNW4G9h9ov0Wd/VC4i6Q6ishYuGnxoTfLyBg7p9SpvwrN5waOS+gy2P+861BAGtYgDgYI59u+b42Pbscf+r4ul72xNp7i3pcszx5rx4pc9YZd86MpsZ3905j4aikRAb20TndyAl8/ZZPDE2/xj3B/7Xp4rFG5jr/K5l58Tr3bBt9P/2Rf+4LQqXfDr/fBJc/BzUsaBklfUNrykf2Z57xfxnG2BmY8tqfZpNv9i2QVbPI3G1UV26S3byr7Tlk2mO5s1AU4Z7F/cOL+bSHrfhvsOIh5Ts8l3+tkEfkgJCVqxfbnbiTHpDN5cKat5nawQTOqleoysOH0HC2p6xA4/Td2jqnA1QHH3mC7zQb2/PE1e/kWbBp6oV3H/Ghq2j0nAuIf7d73tKZXJQRbg/Dp3NMmyZN7wZjr7M3e14wFdjp3d6Q/Z1iw3r8vwhnQmjXKBry0AfZ19xNsj6Fdzqy842+yj6zRzc85lTXK9izz1NrX+evsz4R0f02qc7ZdBOvCp+yswjWltiYE/gT30AttWQee44yXaSLvkL/OH7QDF7dqQcE2MaU5PZcAMMYU05FGUudvgKoSPIXb2evqyuieh+kOp1R7c9LP4PvvN9wW29nfXOPjCxC+fInLbfMNR9P0ldQdfvCRnU79SGI6+5/HpdglbNMG2Gnbp/+9Ye3EN3YhNtnfE8zHVzPw1XR8tZgug+24D5/ssTDld4efkFDEvyoi+BeXSszwB4gkJ+dy3CW2uQ78MwHvXuYvyw0Lbc8tX3OeuGwHhRGX+t975GW2i/COz5sv0/8g2K/AXhHpYYzZBSAivWgypLVT/7D/uDJxsTb5UiLd/2MXQqXao4NNTBn/2/s0nm6kOYE1iLhU2+Tjjm762GynFiNie43lrfLvyxhuex75ug5Pus0OwItPtY8ffmbHPwU7K/Oo79mR5LlLbU8ocduZeA8GiMBu8s7NP3899D0Vdq+wY0tiA4Kfb8R9fLqtUUTFw6qX/GXvfkLYA8SvgM9F5BNspuYkIMiRKG1cXdXBp6u8feDkZtpDleroBk21I6cDcxWhFHgTjU1uuputT2AvqJTeNkl9zu/tCPETf2J7TPmWgE3pYx8+mSOaToI3JybJTtT52g9sgEhItzWGzJH2Jh/Y9BXfxX7WpvdtD6icxbbWFcj3+/QF3sCypQ20k0UG5l5aULBTbcwVkTHYoLACeBOoOvxZ7USl7fc8N+1aflV4Ol8M6xXe8ijVWiV0sYshHSu+GoS4GjY3BbruI38y2Gfy3ba7a99T/cngwNHbLcW3foevyS22M9zRaPlYERg+w650+Oz5dmzECY2mWI9PtYHFN11LYjdbU3JF2G2hKLsj2Kk2fgDcBnQHVgLjgUU0XIK0fXIGxrxXkMoZw3sSE6njHpRqFXxBITa5+ZHj2Sccui190NEv3fptxAfZ5DZiBnzysK3VXPJc01OzTJ/pH03vctnmrsjYkM/pFGwT023ACcBiY8ypIjII+F3oitWKVBYBsLs2niv7HqYKq5Q6tnw1iNhW+v+ycQ2iOal94aKn7YDAjGFNHzPgrIavT/8/O04jxIINENXGmGoRQUSijTEbRKRjTELk1CD204mR2dp7SalWIyreNrMcLvcQTr4p1INJ2o+YcXTvPXjq0ZfnWwg2QOQ64yDeBOaJSDGwM3TFakWcHER9TAq9UuPCXBil1EEithYR18w4iXALtgbRigWbpL7QeXqviCwAkoC5IStVa3KgkHrc9M3OQtrIHO5KdRg9J/nno2pt0gbYgXettXxBOOqhwMaYT0JRkNaqujSfMpPImF6ttBqrVEd2yX+PfEy4xKc2nLqjDdK5Io6gpHAvxSaRSf3Swl0UpZQ6pnRI8BFUlxZQKkkMz0o68sFKKdWOaIA4AndVEa6ENCJ0eg2lVAejd73DqKqpI8lbTExSx5mXUCmlfDRAHEbRmo/oJFXUdmtiNKZSSrVzGiAOI2LVC5SZONxDjs2gFKWUak1CGiBEZIqIbBSRLSJyyCxeInKNiBSIyErn8YOAfZ6A7XNCWc4mVeSTlvMBczwT6N5Fu7gqpTqekHVzFRE3MBM4E8gFlojIHGPMukaHvmyMubmJt6gyxowMVfmO6PNHcXnreU6mckVC6Oc8UUqp1iaUNYixwBZjzDZjTC3wEjA9hJ/Xcjz1sOwZvk44FU9yHx1BrZTqkEIZILKAnIDXuc62xi4WkVUiMltEAlb/JkZElorIYhG5oKkPEJEbnGOWFhQUtFzJy/dC3QG+9g4iO0XnX1JKdUzhTlK/DfQyxowA5gHPBuzraYwZA1wOPCoifRufbIx5yhgzxhgzpkuXLi1XqlIb19ZWdiI7Obbl3lcppdqQUAaI3UBgjaC7s+0gY0yRMca33NM/gdEB+3Y7P7cBC4HjQ1jWhkpsgNhSm0zf9IRj9rFKKdWahDJALAH6i0hvEYkCLgUa9EYSkcyAl9OA9c72ZBGJdp6nAZOAxsnt0CndBcBuk8bAronH7GOVUqo1CVkvJmNMvYjcDHwAuIFZxpi1InI/sNQYMwe4VUSmAfXAfuAa5/TBwJMi4sUGsYeb6P0UOiU5VEUmU10dzcAMDRBKqY4ppLO5GmPeA95rtO03Ac/vAu5q4rwvgeGhLNthleZQ4E6na6doOsdpF1elVMcU7iR161Sayy5PKgMzOoW7JEopFTYaIBozBlOSw+aazgzS5iWlVAemAaKx0hykvootngyGZGoNQinVcWmAaGzfWgDWe3swtJsGCKVUx6UBorG8NQDsjOhJny46BkIp1XHpmtSN7VtDnjuTHuldcbt0DialVMelNYhGzL41rKnP1uYlpVSHpwEiUH0NFG1lrac7Q7slhbs0SikVVhogApXnIRhyTRrDNEAopTo4DRCByvMAKCSFARmaoFZKdWwaIAKV7wUgJqUb0RHuMBdGKaXCSwNEAOMEiC7deoW3IEop1QpoN9cAB4pyiTRuemdnH/lgpZRq5zRABCgryMFDMkOzOoe7KEopFXYaIALUleylwCQzRMdAKKWU5iACRVTuoyIyjYRojZtKKaUBIkBiXSEmMSPcxVBKqVZBA4SjrDifRCqJTs4Kd1GUUqpV0ADhKFvyMgDePqeHuSRKKdU6aIBwxK1/lQ3ebNL6jQl3UZRSqlXQAAFQmktK8Te86ZlEz7T4cJdGKaVaBQ0QAFsXALAmbhwxkTrFhlJKgQYIa9tCil3JeLsMDndJlFKq1dAA4fXCtoV86R1GL11iVCmlDtIAUZqDqa9ifu1QeqTEhbs0SinVamiASO5JwY0becc7XkdQK6VUAA0QQI3XTQ1RREfor0MppXz0jgjUerwARGmAUEqpg/SOCNTU2QChq8gppZSfBgigpt4DoE1MSikVQO+IQG29rwahvw6llPLROyJQ4wsQkfrrUEopH70j4q9BRLk1B6GUUj4aINAahFJKNUXviPiT1FFu/XUopZSP3hEJSFJrDUIppQ7SOyL+JiatQSillF9I74giMkVENorIFhG5s4n914hIgYisdB4/CNh3tYhsdh5Xh7Kc/hqEJqmVUsonZLPTiYgbmAmcCeQCS0RkjjFmXaNDXzbG3Nzo3BTgHmAMYIBlzrnFoSirDpRTSqlDhfKOOBbYYozZZoypBV4Cpgd57tnAPGPMficozAOmhKic1NR7EYEIl4TqI5RSqs0JZYDIAnICXuc62xq7WERWichsEck+mnNF5AYRWSoiSwsKCr51QWvrvURHuBDRAKGUUj7hblN5G+hljBmBrSU8ezQnG2OeMsaMMcaM6dKly7cuRE29VxPUSinVSCjviruB7IDX3Z1tBxljiowxNc7LfwKjgz23JdXUezVBrZRSjYQyQCwB+otIbxGJAi4F5gQeICKZAS+nAeud5x8AZ4lIsogkA2c520Kipt6jCWqllGokZL2YjDH1InIz9sbuBmYZY9aKyP3AUmPMHOBWEZkG1AP7gWucc/eLyAPYIANwvzFmf6jKWlPv1cWClFKqkZAuwmyMeQ94r9G23wQ8vwu4q5lzZwGzQlk+H5uk1iYmpZQKpF+b0RqEUko1Re+KQK3mIJRS6hB6V8TpxaQBQimlGtC7IlBTpwFCKaUa07siUOvRJLVSSjWmAQI7DkKT1Eop1ZDeFfHPxaSUUspP74poN1ellGqK3hXRJLVSSjVF74pokloppZrS4QNEvceLx2u0iUkppRrp8HfFWo+zHrUGCKWUaqDD3xVr6myA0BqEUko11OHvii4RzhuRSZ8uCeEuilJKtSohne67LUiKi2Tm5aPCXQyllGp1OnwNQimlVNM0QCillGqSBgillFJN0gChlFKqSRoglFJKNUkDhFJKqSZpgFBKKdUkDRBKKaWaJMaYcJehRYhIAbDzf3iLNKCwhYoTbu3lWtrLdYBeS2ul1wI9jTFdmtrRDR5UuAAABjVJREFUbgLE/0pElhpjxoS7HC2hvVxLe7kO0GtprfRaDk+bmJRSSjVJA4RSSqkmaYDweyrcBWhB7eVa2st1gF5La6XXchiag1BKKdUkrUEopZRqkgYIpZRSTerwAUJEpojIRhHZIiJ3hrs8R0tEdojIahFZKSJLnW0pIjJPRDY7P5PDXc6miMgsEckXkTUB25osu1iPOX+nVSLSqlZ5auZa7hWR3c7fZqWInBuw7y7nWjaKyNnhKXXTRCRbRBaIyDoRWSsitznb29Tf5jDX0eb+LiISIyJfi8g3zrXc52zvLSJfOWV+WUSinO3Rzustzv5e3+qDjTEd9gG4ga1AHyAK+AYYEu5yHeU17ADSGm37A3Cn8/xO4PfhLmczZT8ZGAWsOVLZgXOB9wEBxgNfhbv8QVzLvcDPmzh2iPNvLRro7fwbdIf7GgLKlwmMcp4nApucMrepv81hrqPN/V2c322C8zwS+Mr5Xb8CXOpsfwL4sfP8RuAJ5/mlwMvf5nM7eg1iLLDFGLPNGFMLvARMD3OZWsJ04Fnn+bPABWEsS7OMMZ8C+xttbq7s04H/GGsx0FlEMo9NSY+smWtpznTgJWNMjTFmO7AF+2+xVTDG7DXGLHeelwPrgSza2N/mMNfRnFb7d3F+txXOy0jnYYDTgNnO9sZ/E9/fajZwuojI0X5uRw8QWUBOwOtcDv8PqDUywIciskxEbnC2dTXG7HWe5wFdw1O0b6W5srfVv9XNTrPLrICmvjZzLU7TxPHYb6xt9m/T6DqgDf5dRMQtIiuBfGAetoZTYoypdw4JLO/Ba3H2lwKpR/uZHT1AtAcnGmNGAecAN4nIyYE7ja1jtsm+zG257I7Hgb7ASGAv8OfwFufoiEgC8BpwuzGmLHBfW/rbNHEdbfLvYozxGGNGAt2xNZtBof7Mjh4gdgPZAa+7O9vaDGPMbudnPvAG9h/OPl8V3/mZH74SHrXmyt7m/lbGmH3Of2ov8DT+5opWfy0iEom9qT5vjHnd2dzm/jZNXUdb/rsAGGNKgAXABGxzXoSzK7C8B6/F2Z8EFB3tZ3X0ALEE6O/0BIjCJnPmhLlMQROReBFJ9D0HzgLWYK/hauewq4G3wlPCb6W5ss8Bvuf0mBkPlAY0d7RKjdrh/7+9u3mxKY7jOP7+SI2nIkXJAoOFFIosDKVkwYoakYeFLG3sJE/lD2A1ZRYWnpKIjaWhKQsNMZ4fs7KykaJIfC1+39E1ncudwRyTz6tu3fs7vznz/fXr9L3nd879no2UuYEyli15p8kcYD7QN9LxNZNr1SeAJxFxtGHTqJqbZuMYjfMiaZqkKfl+PLCWck3lOtCZ3QbPycBcdQLX8qxvaOq+Ol/3i3IHxnPKet7+uuMZYuztlLsu7gGPBuKnrDX2AC+Aq8DUumNtEv85yin+Z8r66a5msVPu4ujKeXoALKs7/hbGcjpjvZ8H7IyG/vtzLM+AdXXHP2gsKynLR/eB/nytH21z85NxjLp5ARYBdzPmh8ChbG+nJLGXwAWgLdvH5eeXub19OP/XpTbMzKzS/77EZGZmTThBmJlZJScIMzOr5ARhZmaVnCDMzKySE4TZP0DSaklX6o7DrJEThJmZVXKCMBsCSduzLn+/pO4soPZe0rGs098jaVr2XSLpZhaFu9zw/IR5kq5mbf87kubm7idJuijpqaSzw6m+afYnOUGYtUjSAmAz0BGlaNoXYBswEbgdEQuBXuBw/skpYG9ELKL8cneg/SzQFRGLgRWUX2BDqTa6h/Jcgnag468Pyuwnxv66i5mlNcBS4FZ+uR9PKVj3FTiffc4AlyRNBqZERG+2nwQuZO2smRFxGSAiPgLk/voi4nV+7gdmAzf+/rDMqjlBmLVOwMmI2PdDo3RwUL/h1q/51PD+Cz4+rWZeYjJrXQ/QKWk6fH9G8yzKcTRQUXMrcCMi3gFvJa3K9h1Ab5Qnm72WtCH30SZpwoiOwqxF/oZi1qKIeCzpAOUJfmMolVt3Ax+A5bntDeU6BZRyy8czAbwCdmb7DqBb0pHcx6YRHIZZy1zN1ew3SXofEZPqjsPsT/MSk5mZVfIZhJmZVfIZhJmZVXKCMDOzSk4QZmZWyQnCzMwqOUGYmVmlb2s9eZ1Sf0mmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pfn.model.history.history['acc'])\n",
    "plt.plot(pfn.model.history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get predictions on test data\n",
    "preds = pfn.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 2 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c569374dfc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mY_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# add 3 1s to the start of each event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_concat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# cut off no-jet jet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 2 with size 3"
     ]
    }
   ],
   "source": [
    "Y_concat = np.concatenate((np.ones((len(Y_test), 3)), Y_test), axis=1).astype(int)  # add 3 1s to the start of each event\n",
    "truth = Y_concat[:,:-1]  # cut off no-jet jet\n",
    "tag = X_test[:,:,3].astype(int)\n",
    "\n",
    "\n",
    "preds_max = np.argmax(preds, axis=1) + 3\n",
    "preds_arr = np.zeros((len(preds), len(preds[0])+3), dtype=int)\n",
    "for i, p in enumerate(preds_max):\n",
    "    preds_arr[i][p] = 1\n",
    "\n",
    "# make sure we're getting the right accuracy\n",
    "n_events = len(preds_arr)\n",
    "n_correct = 0\n",
    "for i in range(len(preds_arr)):\n",
    "    if all(preds_arr[i][3:] == Y_test[i]):\n",
    "        n_correct += 1\n",
    "    \n",
    "# chop off 'no jet' jet\n",
    "selections = preds_arr[:,:-1].astype(int)\n",
    "\n",
    "\n",
    "# ensure accuracy again\n",
    "n_events = len(truth)\n",
    "n_correct = 0\n",
    "for i in range(len(truth)):\n",
    "    if all(truth[i][3:] == selections[i][3:]):\n",
    "        n_correct += 1\n",
    "\n",
    "import importlib\n",
    "importlib.reload(tools)\n",
    "\n",
    "#print(truth.shape, tag.shape, selections.shape)\n",
    "tools.evaluate_model(truth, tag, selections)\n",
    "\n",
    "plt.title(\"truth\")\n",
    "plt.hist(y.argmax(axis=1), bins=15, density=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title(\"selections\")\n",
    "plt.hist(preds_max, bins=15, density=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
