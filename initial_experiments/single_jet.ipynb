{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Jet At A Time\n",
    "\n",
    "Maybe it's useful to have a single-jet classifier?\n",
    "\n",
    "Is it even possible to classify jets based on $p_T, \\eta, \\phi$ alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import uproot as ur\n",
    "import uproot_methods as urm\n",
    "import numpy as np\n",
    "import awkward\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import tools\n",
    "\n",
    "filename = 'user.jagrundy.20736236._000001.MiniNTuple.root'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ntuple, get the data we need from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting data by tag\n"
     ]
    }
   ],
   "source": [
    "s_table = tools.open_file(filename, sort_by=\"tag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312386\n"
     ]
    }
   ],
   "source": [
    "nj4 = s_table.njets>=4   # >=4 jets exist\n",
    "nb4 = s_table.nbjets>=4  # >=4 b jets really exist\n",
    "nt3 = s_table.nbtags==3  # 3 b tags\n",
    "nb4nt3 = nb4 & nt3  # >=4 bjets, exactly 3 are tagged\n",
    "nj4nt3 = nj4 & nt3  # >=4 jets, exactly 3 are tagged\n",
    "\n",
    "events = s_table[nj4nt3]\n",
    "n_events = len(events)\n",
    "print(n_events)\n",
    "\n",
    "pt = events[\"resolved_lv\"].pt\n",
    "eta = events[\"resolved_lv\"].eta\n",
    "phi = events[\"resolved_lv\"].phi\n",
    "truth = events[\"truth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten everything so we just have jets, no longer organized as events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# also reshape so we have arrays that look like [[1], [2], [3], ...]\n",
    "pt = pt.flatten().reshape(-1, 1)\n",
    "eta = eta.flatten().reshape(-1, 1)\n",
    "phi = phi.flatten().reshape(-1, 1)\n",
    "truth = truth.flatten().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then prepare for keras stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a True b-jet:\n",
      "pt [55.581104] eta [2.3661427] phi [0.85040945]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print an example\n",
    "i=1\n",
    "print(\"This is a\", bool(truth[i]), \"b-jet:\")\n",
    "print(\"pt\", pt[i], \"eta\", eta[i], \"phi\", phi[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data to be keras-friendly\n",
    "scaler_pt = StandardScaler()\n",
    "scaler_eta = StandardScaler()\n",
    "scaler_phi = StandardScaler()\n",
    "\n",
    "# s for scaled\n",
    "s_pt = scaler_pt.fit_transform(pt)\n",
    "s_eta = scaler_eta.fit_transform(eta)\n",
    "s_phi = scaler_phi.fit_transform(phi)\n",
    "\n",
    "# stack pt, eta, phi for input into model\n",
    "s_in = np.column_stack((s_pt, s_eta, s_phi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, validation, and testing subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split missed_jet into training, validation, testing subsets\n",
    "train, val, test = tools.splitTVT(truth, trainfrac=0.7, testfrac=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I experimented a little with having different layers and this ended up being the best trade off of speed to quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=3, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# compile model\n",
    "optimizer = Adam(lr=5e-5)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1249483 samples, validate on 178499 samples\n",
      "Epoch 1/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.6854 - acc: 0.6248 - val_loss: 0.6757 - val_acc: 0.6231\n",
      "Epoch 2/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.6614 - acc: 0.6247 - val_loss: 0.6479 - val_acc: 0.6231\n",
      "Epoch 3/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.6369 - acc: 0.6247 - val_loss: 0.6282 - val_acc: 0.6231\n",
      "Epoch 4/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.6194 - acc: 0.6247 - val_loss: 0.6124 - val_acc: 0.6231\n",
      "Epoch 5/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.6061 - acc: 0.6335 - val_loss: 0.6020 - val_acc: 0.6964\n",
      "Epoch 6/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5989 - acc: 0.7221 - val_loss: 0.5971 - val_acc: 0.7320\n",
      "Epoch 7/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5949 - acc: 0.7337 - val_loss: 0.5934 - val_acc: 0.7335\n",
      "Epoch 8/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5911 - acc: 0.7332 - val_loss: 0.5891 - val_acc: 0.7315\n",
      "Epoch 9/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5867 - acc: 0.7306 - val_loss: 0.5843 - val_acc: 0.7297\n",
      "Epoch 10/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5812 - acc: 0.7295 - val_loss: 0.5782 - val_acc: 0.7291\n",
      "Epoch 11/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5749 - acc: 0.7292 - val_loss: 0.5718 - val_acc: 0.7288\n",
      "Epoch 12/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5686 - acc: 0.7294 - val_loss: 0.5657 - val_acc: 0.7293\n",
      "Epoch 13/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5632 - acc: 0.7298 - val_loss: 0.5609 - val_acc: 0.7298\n",
      "Epoch 14/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5591 - acc: 0.7303 - val_loss: 0.5577 - val_acc: 0.7301\n",
      "Epoch 15/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5566 - acc: 0.7308 - val_loss: 0.5558 - val_acc: 0.7313\n",
      "Epoch 16/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5552 - acc: 0.7313 - val_loss: 0.5548 - val_acc: 0.7318\n",
      "Epoch 17/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5544 - acc: 0.7319 - val_loss: 0.5541 - val_acc: 0.7318\n",
      "Epoch 18/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5538 - acc: 0.7322 - val_loss: 0.5535 - val_acc: 0.7321\n",
      "Epoch 19/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5532 - acc: 0.7326 - val_loss: 0.5530 - val_acc: 0.7321\n",
      "Epoch 20/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5527 - acc: 0.7330 - val_loss: 0.5525 - val_acc: 0.7325\n",
      "Epoch 21/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5522 - acc: 0.7334 - val_loss: 0.5521 - val_acc: 0.7330\n",
      "Epoch 22/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5518 - acc: 0.7336 - val_loss: 0.5517 - val_acc: 0.7333\n",
      "Epoch 23/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5514 - acc: 0.7338 - val_loss: 0.5514 - val_acc: 0.7335\n",
      "Epoch 24/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5511 - acc: 0.7339 - val_loss: 0.5511 - val_acc: 0.7334\n",
      "Epoch 25/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5508 - acc: 0.7341 - val_loss: 0.5508 - val_acc: 0.7334\n",
      "Epoch 26/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5505 - acc: 0.7341 - val_loss: 0.5506 - val_acc: 0.7337\n",
      "Epoch 27/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5503 - acc: 0.7342 - val_loss: 0.5504 - val_acc: 0.7338\n",
      "Epoch 28/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5500 - acc: 0.7342 - val_loss: 0.5501 - val_acc: 0.7339\n",
      "Epoch 29/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5498 - acc: 0.7343 - val_loss: 0.5499 - val_acc: 0.7341\n",
      "Epoch 30/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5496 - acc: 0.7343 - val_loss: 0.5497 - val_acc: 0.7341\n",
      "Epoch 31/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5495 - acc: 0.7343 - val_loss: 0.5496 - val_acc: 0.7340\n",
      "Epoch 32/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5493 - acc: 0.7342 - val_loss: 0.5494 - val_acc: 0.7339\n",
      "Epoch 33/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5492 - acc: 0.7343 - val_loss: 0.5493 - val_acc: 0.7339\n",
      "Epoch 34/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5491 - acc: 0.7343 - val_loss: 0.5492 - val_acc: 0.7340\n",
      "Epoch 35/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5490 - acc: 0.7343 - val_loss: 0.5491 - val_acc: 0.7339\n",
      "Epoch 36/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5488 - acc: 0.7343 - val_loss: 0.5490 - val_acc: 0.7341\n",
      "Epoch 37/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5487 - acc: 0.7343 - val_loss: 0.5489 - val_acc: 0.7341\n",
      "Epoch 38/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5486 - acc: 0.7343 - val_loss: 0.5488 - val_acc: 0.7339\n",
      "Epoch 39/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5486 - acc: 0.7343 - val_loss: 0.5487 - val_acc: 0.7339\n",
      "Epoch 40/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5485 - acc: 0.7342 - val_loss: 0.5487 - val_acc: 0.7340\n",
      "Epoch 41/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5484 - acc: 0.7343 - val_loss: 0.5485 - val_acc: 0.7337\n",
      "Epoch 42/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5483 - acc: 0.7343 - val_loss: 0.5485 - val_acc: 0.7339\n",
      "Epoch 43/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5483 - acc: 0.7343 - val_loss: 0.5484 - val_acc: 0.7338\n",
      "Epoch 44/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5482 - acc: 0.7343 - val_loss: 0.5484 - val_acc: 0.7340\n",
      "Epoch 45/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5481 - acc: 0.7342 - val_loss: 0.5483 - val_acc: 0.7336\n",
      "Epoch 46/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5481 - acc: 0.7342 - val_loss: 0.5483 - val_acc: 0.7338\n",
      "Epoch 47/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5480 - acc: 0.7343 - val_loss: 0.5482 - val_acc: 0.7339\n",
      "Epoch 48/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5480 - acc: 0.7342 - val_loss: 0.5481 - val_acc: 0.7336\n",
      "Epoch 49/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5479 - acc: 0.7342 - val_loss: 0.5482 - val_acc: 0.7338\n",
      "Epoch 50/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5479 - acc: 0.7342 - val_loss: 0.5481 - val_acc: 0.7339\n",
      "Epoch 51/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5478 - acc: 0.7342 - val_loss: 0.5480 - val_acc: 0.7339\n",
      "Epoch 52/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5478 - acc: 0.7342 - val_loss: 0.5480 - val_acc: 0.7338\n",
      "Epoch 53/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5477 - acc: 0.7342 - val_loss: 0.5479 - val_acc: 0.7338\n",
      "Epoch 54/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5477 - acc: 0.7343 - val_loss: 0.5479 - val_acc: 0.7337\n",
      "Epoch 55/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5476 - acc: 0.7342 - val_loss: 0.5479 - val_acc: 0.7336\n",
      "Epoch 56/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5476 - acc: 0.7342 - val_loss: 0.5479 - val_acc: 0.7337\n",
      "Epoch 57/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5476 - acc: 0.7343 - val_loss: 0.5479 - val_acc: 0.7339\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5475 - acc: 0.7342 - val_loss: 0.5478 - val_acc: 0.7339\n",
      "Epoch 59/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5475 - acc: 0.7342 - val_loss: 0.5477 - val_acc: 0.7338\n",
      "Epoch 60/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5475 - acc: 0.7342 - val_loss: 0.5477 - val_acc: 0.7339\n",
      "Epoch 61/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5474 - acc: 0.7342 - val_loss: 0.5478 - val_acc: 0.7339\n",
      "Epoch 62/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5474 - acc: 0.7342 - val_loss: 0.5477 - val_acc: 0.7338\n",
      "Epoch 63/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5474 - acc: 0.7343 - val_loss: 0.5477 - val_acc: 0.7337\n",
      "Epoch 64/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5474 - acc: 0.7343 - val_loss: 0.5476 - val_acc: 0.7337\n",
      "Epoch 65/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5473 - acc: 0.7343 - val_loss: 0.5476 - val_acc: 0.7337\n",
      "Epoch 66/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5473 - acc: 0.7343 - val_loss: 0.5476 - val_acc: 0.7338\n",
      "Epoch 67/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5473 - acc: 0.7342 - val_loss: 0.5476 - val_acc: 0.7340\n",
      "Epoch 68/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5473 - acc: 0.7342 - val_loss: 0.5476 - val_acc: 0.7338\n",
      "Epoch 69/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5473 - acc: 0.7343 - val_loss: 0.5476 - val_acc: 0.7339\n",
      "Epoch 70/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5472 - acc: 0.7342 - val_loss: 0.5475 - val_acc: 0.7336\n",
      "Epoch 71/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5472 - acc: 0.7342 - val_loss: 0.5475 - val_acc: 0.7338\n",
      "Epoch 72/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5472 - acc: 0.7342 - val_loss: 0.5475 - val_acc: 0.7340\n",
      "Epoch 73/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5472 - acc: 0.7343 - val_loss: 0.5475 - val_acc: 0.7339\n",
      "Epoch 74/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5472 - acc: 0.7343 - val_loss: 0.5475 - val_acc: 0.7339\n",
      "Epoch 75/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5471 - acc: 0.7343 - val_loss: 0.5474 - val_acc: 0.7336\n",
      "Epoch 76/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5471 - acc: 0.7343 - val_loss: 0.5475 - val_acc: 0.7339\n",
      "Epoch 77/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5471 - acc: 0.7343 - val_loss: 0.5474 - val_acc: 0.7339\n",
      "Epoch 78/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5471 - acc: 0.7343 - val_loss: 0.5474 - val_acc: 0.7338\n",
      "Epoch 79/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5471 - acc: 0.7343 - val_loss: 0.5474 - val_acc: 0.7339\n",
      "Epoch 80/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5471 - acc: 0.7343 - val_loss: 0.5474 - val_acc: 0.7339\n",
      "Epoch 81/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5470 - acc: 0.7342 - val_loss: 0.5474 - val_acc: 0.7339\n",
      "Epoch 82/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5470 - acc: 0.7342 - val_loss: 0.5474 - val_acc: 0.7338\n",
      "Epoch 83/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5470 - acc: 0.7342 - val_loss: 0.5474 - val_acc: 0.7338\n",
      "Epoch 84/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5470 - acc: 0.7343 - val_loss: 0.5474 - val_acc: 0.7339\n",
      "Epoch 85/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5470 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7338\n",
      "Epoch 86/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5470 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7339\n",
      "Epoch 87/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5470 - acc: 0.7342 - val_loss: 0.5473 - val_acc: 0.7339\n",
      "Epoch 88/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5470 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7340\n",
      "Epoch 89/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5470 - acc: 0.7342 - val_loss: 0.5473 - val_acc: 0.7339\n",
      "Epoch 90/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5469 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7340\n",
      "Epoch 91/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5469 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7339\n",
      "Epoch 92/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5469 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7339\n",
      "Epoch 93/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5469 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7338\n",
      "Epoch 94/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5469 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7339\n",
      "Epoch 95/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5469 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7340\n",
      "Epoch 96/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5469 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7339\n",
      "Epoch 97/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5469 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7339\n",
      "Epoch 98/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5469 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7341\n",
      "Epoch 99/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5469 - acc: 0.7343 - val_loss: 0.5473 - val_acc: 0.7339\n",
      "Epoch 100/100\n",
      "1249483/1249483 [==============================] - 2s 1us/step - loss: 0.5469 - acc: 0.7343 - val_loss: 0.5472 - val_acc: 0.7340\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(s_in[train], truth[train],\n",
    "                    validation_data=(s_in[val], truth[val]),\n",
    "                    epochs = 100, batch_size = 2000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = model.predict_classes(s_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(selection, bins=100)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"model output\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.187476925472986\n"
     ]
    }
   ],
   "source": [
    "selection = selection.flatten()\n",
    "truth = truth.flatten()\n",
    "\n",
    "n = len(selection)\n",
    "\n",
    "correct = np.logical_and(selection, truth.flatten())\n",
    "n_correct = np.count_nonzero(correct)\n",
    "\n",
    "percent_correct = n_correct/n*100\n",
    "print(percent_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it seems that was a bit of waste of time. Good try anyway?"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit1bc87f48281d409ab7479945988ac1ab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
