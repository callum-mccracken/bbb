{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Jet At A Time\n",
    "\n",
    "Maybe it's useful to have a single-jet classifier?\n",
    "\n",
    "Is it even possible to classify jets based on $p_T, \\eta, \\phi$ alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import uproot as ur\n",
    "import uproot_methods as urm\n",
    "import numpy as np\n",
    "import awkward\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import tools\n",
    "\n",
    "filename = 'user.jagrundy.20736236._000001.MiniNTuple.root'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ntuple, get the data we need from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sorting data by tag\n"
    }
   ],
   "source": [
    "s_table = tools.open_file(filename, sort_by=\"tag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "312386\n"
    }
   ],
   "source": [
    "nj4 = s_table.njets>=4   # >=4 jets exist\n",
    "nb4 = s_table.nbjets>=4  # >=4 b jets really exist\n",
    "nt3 = s_table.nbtags==3  # 3 b tags\n",
    "nb4nt3 = nb4 & nt3  # >=4 bjets, exactly 3 are tagged\n",
    "nj4nt3 = nj4 & nt3  # >=4 jets, exactly 3 are tagged\n",
    "\n",
    "events = s_table[nj4nt3]\n",
    "n_events = len(events)\n",
    "print(n_events)\n",
    "\n",
    "pt = events[\"resolved_lv\"].pt\n",
    "eta = events[\"resolved_lv\"].eta\n",
    "phi = events[\"resolved_lv\"].phi\n",
    "truth = events[\"truth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten everything so we just have jets, no longer organized as events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# also reshape so we have arrays that look like [[1], [2], [3], ...]\n",
    "pt = pt.flatten().reshape(-1, 1)\n",
    "eta = eta.flatten().reshape(-1, 1)\n",
    "phi = phi.flatten().reshape(-1, 1)\n",
    "truth = truth.flatten().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then prepare for keras stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "This is a True b-jet:\npt [55.581104] eta [2.3661427] phi [0.85040945]\n\n"
    }
   ],
   "source": [
    "# print an example\n",
    "i=1\n",
    "print(\"This is a\", bool(truth[i]), \"b-jet:\")\n",
    "print(\"pt\", pt[i], \"eta\", eta[i], \"phi\", phi[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data to be keras-friendly\n",
    "scaler_pt = StandardScaler()\n",
    "scaler_eta = StandardScaler()\n",
    "scaler_phi = StandardScaler()\n",
    "\n",
    "# s for scaled\n",
    "s_pt = scaler_pt.fit_transform(pt)\n",
    "s_eta = scaler_eta.fit_transform(eta)\n",
    "s_phi = scaler_phi.fit_transform(phi)\n",
    "\n",
    "# stack pt, eta, phi for input into model\n",
    "s_in = np.column_stack((s_pt, s_eta, s_phi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, validation, and testing subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split missed_jet into training, validation, testing subsets\n",
    "train, val, test = tools.splitTVT(truth, trainfrac=0.7, testfrac=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I experimented a little with having different layers and this ended up being the best trade off of speed to quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=3, kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(2, kernel_initializer='normal', activation='softmax'))\n",
    "# compile model\n",
    "optimizer = Adam(lr=5e-5)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 1249483 samples, validate on 178499 samples\nEpoch 1/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5242 - acc: 0.7536 - val_loss: 0.5247 - val_acc: 0.7528\nEpoch 2/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5237 - acc: 0.7537 - val_loss: 0.5241 - val_acc: 0.7531\nEpoch 3/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5231 - acc: 0.7539 - val_loss: 0.5235 - val_acc: 0.7530\nEpoch 4/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5224 - acc: 0.7540 - val_loss: 0.5229 - val_acc: 0.7536\nEpoch 5/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5218 - acc: 0.7543 - val_loss: 0.5222 - val_acc: 0.7535\nEpoch 6/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5211 - acc: 0.7546 - val_loss: 0.5216 - val_acc: 0.7543\nEpoch 7/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5206 - acc: 0.7548 - val_loss: 0.5211 - val_acc: 0.7547\nEpoch 8/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5201 - acc: 0.7551 - val_loss: 0.5206 - val_acc: 0.7543\nEpoch 9/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5197 - acc: 0.7554 - val_loss: 0.5202 - val_acc: 0.7551\nEpoch 10/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5193 - acc: 0.7555 - val_loss: 0.5198 - val_acc: 0.7550\nEpoch 11/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5189 - acc: 0.7556 - val_loss: 0.5194 - val_acc: 0.7552\nEpoch 12/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5186 - acc: 0.7558 - val_loss: 0.5191 - val_acc: 0.7556\nEpoch 13/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5183 - acc: 0.7559 - val_loss: 0.5189 - val_acc: 0.7556\nEpoch 14/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5181 - acc: 0.7561 - val_loss: 0.5186 - val_acc: 0.7558\nEpoch 15/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5178 - acc: 0.7562 - val_loss: 0.5184 - val_acc: 0.7559\nEpoch 16/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5176 - acc: 0.7563 - val_loss: 0.5182 - val_acc: 0.7560\nEpoch 17/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5174 - acc: 0.7564 - val_loss: 0.5180 - val_acc: 0.7562\nEpoch 18/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5172 - acc: 0.7565 - val_loss: 0.5178 - val_acc: 0.7562\nEpoch 19/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5170 - acc: 0.7566 - val_loss: 0.5176 - val_acc: 0.7562\nEpoch 20/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5168 - acc: 0.7566 - val_loss: 0.5175 - val_acc: 0.7565\nEpoch 21/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5167 - acc: 0.7567 - val_loss: 0.5173 - val_acc: 0.7562\nEpoch 22/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5165 - acc: 0.7568 - val_loss: 0.5172 - val_acc: 0.7564\nEpoch 23/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5164 - acc: 0.7569 - val_loss: 0.5170 - val_acc: 0.7566\nEpoch 24/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5162 - acc: 0.7569 - val_loss: 0.5169 - val_acc: 0.7566\nEpoch 25/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5161 - acc: 0.7570 - val_loss: 0.5167 - val_acc: 0.7566\nEpoch 26/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5159 - acc: 0.7570 - val_loss: 0.5165 - val_acc: 0.7566\nEpoch 27/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5157 - acc: 0.7572 - val_loss: 0.5164 - val_acc: 0.7566\nEpoch 28/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5155 - acc: 0.7573 - val_loss: 0.5162 - val_acc: 0.7568\nEpoch 29/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5154 - acc: 0.7573 - val_loss: 0.5160 - val_acc: 0.7570\nEpoch 30/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5152 - acc: 0.7575 - val_loss: 0.5158 - val_acc: 0.7569\nEpoch 31/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5150 - acc: 0.7576 - val_loss: 0.5157 - val_acc: 0.7571\nEpoch 32/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5148 - acc: 0.7577 - val_loss: 0.5155 - val_acc: 0.7570\nEpoch 33/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5146 - acc: 0.7578 - val_loss: 0.5153 - val_acc: 0.7573\nEpoch 34/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5144 - acc: 0.7579 - val_loss: 0.5153 - val_acc: 0.7570\nEpoch 35/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5143 - acc: 0.7581 - val_loss: 0.5150 - val_acc: 0.7575\nEpoch 36/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5141 - acc: 0.7581 - val_loss: 0.5148 - val_acc: 0.7577\nEpoch 37/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5139 - acc: 0.7582 - val_loss: 0.5147 - val_acc: 0.7578\nEpoch 38/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5137 - acc: 0.7583 - val_loss: 0.5144 - val_acc: 0.7579\nEpoch 39/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5135 - acc: 0.7583 - val_loss: 0.5143 - val_acc: 0.7579\nEpoch 40/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5133 - acc: 0.7585 - val_loss: 0.5141 - val_acc: 0.7580\nEpoch 41/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5131 - acc: 0.7586 - val_loss: 0.5139 - val_acc: 0.7582\nEpoch 42/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5130 - acc: 0.7586 - val_loss: 0.5138 - val_acc: 0.7584\nEpoch 43/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5128 - acc: 0.7588 - val_loss: 0.5136 - val_acc: 0.7583\nEpoch 44/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5127 - acc: 0.7588 - val_loss: 0.5135 - val_acc: 0.7585\nEpoch 45/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5125 - acc: 0.7590 - val_loss: 0.5133 - val_acc: 0.7586\nEpoch 46/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5124 - acc: 0.7592 - val_loss: 0.5132 - val_acc: 0.7587\nEpoch 47/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5122 - acc: 0.7592 - val_loss: 0.5130 - val_acc: 0.7588\nEpoch 48/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5121 - acc: 0.7593 - val_loss: 0.5130 - val_acc: 0.7589\nEpoch 49/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5120 - acc: 0.7594 - val_loss: 0.5128 - val_acc: 0.7588\nEpoch 50/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5119 - acc: 0.7596 - val_loss: 0.5127 - val_acc: 0.7592\nEpoch 51/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5117 - acc: 0.7596 - val_loss: 0.5127 - val_acc: 0.7592\nEpoch 52/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5116 - acc: 0.7597 - val_loss: 0.5125 - val_acc: 0.7590\nEpoch 53/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5116 - acc: 0.7598 - val_loss: 0.5123 - val_acc: 0.7593\nEpoch 54/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5115 - acc: 0.7598 - val_loss: 0.5123 - val_acc: 0.7595\nEpoch 55/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5114 - acc: 0.7599 - val_loss: 0.5122 - val_acc: 0.7593\nEpoch 56/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5113 - acc: 0.7599 - val_loss: 0.5121 - val_acc: 0.7596\nEpoch 57/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5112 - acc: 0.7600 - val_loss: 0.5120 - val_acc: 0.7597\nEpoch 58/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5112 - acc: 0.7600 - val_loss: 0.5120 - val_acc: 0.7598\nEpoch 59/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5111 - acc: 0.7602 - val_loss: 0.5119 - val_acc: 0.7598\nEpoch 60/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5111 - acc: 0.7602 - val_loss: 0.5118 - val_acc: 0.7598\nEpoch 61/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5110 - acc: 0.7603 - val_loss: 0.5118 - val_acc: 0.7599\nEpoch 62/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5109 - acc: 0.7604 - val_loss: 0.5117 - val_acc: 0.7600\nEpoch 63/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5109 - acc: 0.7604 - val_loss: 0.5117 - val_acc: 0.7599\nEpoch 64/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5108 - acc: 0.7605 - val_loss: 0.5116 - val_acc: 0.7601\nEpoch 65/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5108 - acc: 0.7606 - val_loss: 0.5115 - val_acc: 0.7600\nEpoch 66/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5107 - acc: 0.7606 - val_loss: 0.5115 - val_acc: 0.7602\nEpoch 67/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5107 - acc: 0.7607 - val_loss: 0.5115 - val_acc: 0.7604\nEpoch 68/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5107 - acc: 0.7607 - val_loss: 0.5114 - val_acc: 0.7602\nEpoch 69/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5106 - acc: 0.7608 - val_loss: 0.5114 - val_acc: 0.7604\nEpoch 70/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5106 - acc: 0.7608 - val_loss: 0.5113 - val_acc: 0.7603\nEpoch 71/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5105 - acc: 0.7609 - val_loss: 0.5113 - val_acc: 0.7603\nEpoch 72/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5105 - acc: 0.7609 - val_loss: 0.5112 - val_acc: 0.7606\nEpoch 73/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5105 - acc: 0.7609 - val_loss: 0.5112 - val_acc: 0.7602\nEpoch 74/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5104 - acc: 0.7610 - val_loss: 0.5111 - val_acc: 0.7605\nEpoch 75/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5104 - acc: 0.7610 - val_loss: 0.5113 - val_acc: 0.7606\nEpoch 76/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5104 - acc: 0.7611 - val_loss: 0.5111 - val_acc: 0.7606\nEpoch 77/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5103 - acc: 0.7611 - val_loss: 0.5111 - val_acc: 0.7607\nEpoch 78/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5103 - acc: 0.7612 - val_loss: 0.5110 - val_acc: 0.7606\nEpoch 79/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5103 - acc: 0.7613 - val_loss: 0.5110 - val_acc: 0.7606\nEpoch 80/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5102 - acc: 0.7612 - val_loss: 0.5111 - val_acc: 0.7607\nEpoch 81/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5102 - acc: 0.7613 - val_loss: 0.5109 - val_acc: 0.7606\nEpoch 82/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5102 - acc: 0.7612 - val_loss: 0.5109 - val_acc: 0.7608\nEpoch 83/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5101 - acc: 0.7613 - val_loss: 0.5108 - val_acc: 0.7607\nEpoch 84/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5101 - acc: 0.7614 - val_loss: 0.5108 - val_acc: 0.7609\nEpoch 85/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5101 - acc: 0.7614 - val_loss: 0.5108 - val_acc: 0.7608\nEpoch 86/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5101 - acc: 0.7613 - val_loss: 0.5108 - val_acc: 0.7610\nEpoch 87/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5100 - acc: 0.7615 - val_loss: 0.5107 - val_acc: 0.7609\nEpoch 88/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5100 - acc: 0.7615 - val_loss: 0.5107 - val_acc: 0.7609\nEpoch 89/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5100 - acc: 0.7615 - val_loss: 0.5107 - val_acc: 0.7609\nEpoch 90/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5100 - acc: 0.7616 - val_loss: 0.5106 - val_acc: 0.7611\nEpoch 91/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5099 - acc: 0.7615 - val_loss: 0.5108 - val_acc: 0.7609\nEpoch 92/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5099 - acc: 0.7615 - val_loss: 0.5106 - val_acc: 0.7612\nEpoch 93/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5099 - acc: 0.7615 - val_loss: 0.5106 - val_acc: 0.7611\nEpoch 94/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5098 - acc: 0.7616 - val_loss: 0.5105 - val_acc: 0.7612\nEpoch 95/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5098 - acc: 0.7616 - val_loss: 0.5105 - val_acc: 0.7612\nEpoch 96/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5098 - acc: 0.7616 - val_loss: 0.5104 - val_acc: 0.7613\nEpoch 97/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5098 - acc: 0.7618 - val_loss: 0.5105 - val_acc: 0.7610\nEpoch 98/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5097 - acc: 0.7617 - val_loss: 0.5104 - val_acc: 0.7613\nEpoch 99/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5097 - acc: 0.7617 - val_loss: 0.5104 - val_acc: 0.7613\nEpoch 100/100\n1249483/1249483 [==============================] - 2s 2us/step - loss: 0.5097 - acc: 0.7617 - val_loss: 0.5104 - val_acc: 0.7611\n198.51475882530212\n"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "truth_binary = to_categorical(truth)\n",
    "\n",
    "import time\n",
    "before = time.time()\n",
    "history = model.fit(s_in[train], truth_binary[train], validation_data=(s_in[val], truth_binary[val]), epochs = 100, batch_size = 2000, verbose = 1)\n",
    "\n",
    "timedelta = time.time() - before\n",
    "print(timedelta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_score = model.predict(s_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection[i] = 0 or 1, model's output \n",
    "selection = np.argmax(nn_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "55.487611871314726\n"
    }
   ],
   "source": [
    "n_test = len(selection[test])\n",
    "correct = np.logical_and(selection[test], truth[test].flatten())\n",
    "n_correct = np.count_nonzero(correct)\n",
    "\n",
    "percent_correct = n_correct/n_test*100\n",
    "print(percent_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it seems that was a bit of waste of time. Good try anyway?"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit1bc87f48281d409ab7479945988ac1ab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}